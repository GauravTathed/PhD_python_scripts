{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5473f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import subprocess, sys, time, math, os\n",
    "\n",
    "def run(x):\n",
    "    try:\n",
    "        p = subprocess.run(x, shell=True, capture_output=True, text=True, timeout=8)\n",
    "        if p.stdout: print(p.stdout.strip())\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "gpu = \"None\"\n",
    "backend = None\n",
    "has_cuda = False\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    try:\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "        import torch.backends.cuda\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    except Exception:\n",
    "        pass\n",
    "    has_cuda = torch.cuda.is_available()\n",
    "    if has_cuda:\n",
    "        gpu = torch.cuda.get_device_name(0)\n",
    "        backend = \"torch\"\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print(f\"Backend: {backend}\")\n",
    "print(f\"GPU: {gpu}\")\n",
    "run(\"nvidia-smi -L\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773a46c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time, math\n",
    "\n",
    "import torch\n",
    "\n",
    "def pick_sizes():\n",
    "    if torch.cuda.is_available():\n",
    "        free_b, total_b = torch.cuda.mem_get_info()\n",
    "        # Aim to use at most ~40% of free VRAM for mm (3*N*N*bytes). Stay capped at 8192 for CPU friendliness.\n",
    "        budget = int(min(free_b*0.4, total_b*0.25))\n",
    "        # Assume fp32 worst case (4 bytes) and 3 matrices in memory\n",
    "        N = int((budget / (3*4))**0.5)\n",
    "        N = max(2048, min(N, 8192))\n",
    "        N = (N//128)*128\n",
    "        # FFT uses complex64; be conservative on size\n",
    "        M = max(1024, min(4096, N))\n",
    "        M = (M//128)*128\n",
    "    else:\n",
    "        # Safe CPU-only defaults\n",
    "        N = 4096\n",
    "        M = 2048\n",
    "    return N, M\n",
    "\n",
    "def gflops_mm(N, seconds):\n",
    "    ops = 2.0 * N * N * N\n",
    "    return (ops / 1e9) / seconds\n",
    "\n",
    "def bench_mm(device, N, repeats=5, dtype=torch.float32, amp_dtype=None):\n",
    "    # If amp_dtype is set, run under autocast to hit Tensor Cores\n",
    "    a = torch.randn((N,N), device=device, dtype=dtype)\n",
    "    b = torch.randn((N,N), device=device, dtype=dtype)\n",
    "    if device.type == \"cuda\": torch.cuda.synchronize()\n",
    "    # Warmup\n",
    "    for _ in range(2):\n",
    "        _ = a @ b\n",
    "    if device.type == \"cuda\": torch.cuda.synchronize()\n",
    "\n",
    "    if amp_dtype is not None and device.type == \"cuda\":\n",
    "        runs = []\n",
    "        for _ in range(repeats):\n",
    "            t0 = time.perf_counter()\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=amp_dtype):\n",
    "                _ = a @ b\n",
    "            torch.cuda.synchronize()\n",
    "            runs.append(time.perf_counter()-t0)\n",
    "        per = sum(runs)/len(runs)\n",
    "    else:\n",
    "        t0 = time.perf_counter()\n",
    "        for _ in range(repeats):\n",
    "            _ = a @ b\n",
    "        if device.type == \"cuda\": torch.cuda.synchronize()\n",
    "        per = (time.perf_counter()-t0)/repeats\n",
    "    return per, gflops_mm(N, per)\n",
    "\n",
    "def bench_elem(device, N, repeats=5, dtype=torch.float32):\n",
    "    x = torch.randn((N,N), device=device, dtype=dtype)\n",
    "    if device.type == \"cuda\": torch.cuda.synchronize()\n",
    "    # Warmup\n",
    "    y = torch.sin(x) + torch.exp(x) + torch.tanh(x)\n",
    "    if device.type == \"cuda\": torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(repeats):\n",
    "        y = torch.sin(y) + torch.exp(y) + torch.tanh(y)\n",
    "    if device.type == \"cuda\": torch.cuda.synchronize()\n",
    "    per = (time.perf_counter()-t0)/repeats\n",
    "    return per\n",
    "\n",
    "def bench_fft2(device, M, repeats=3, complex_dtype=torch.complex64):\n",
    "    x = torch.randn((M,M), device=device, dtype=torch.float32)\n",
    "    x = x.to(complex_dtype) + 1j*torch.zeros_like(x, dtype=complex_dtype)\n",
    "    if device.type == \"cuda\": torch.cuda.synchronize()\n",
    "    # Warmup\n",
    "    _ = torch.fft.fft2(x)\n",
    "    if device.type == \"cuda\": torch.cuda.synchronize()\n",
    "    t0 = time.perf_counter()\n",
    "    for _ in range(repeats):\n",
    "        _ = torch.fft.fft2(x)\n",
    "    if device.type == \"cuda\": torch.cuda.synchronize()\n",
    "    per = (time.perf_counter()-t0)/repeats\n",
    "    return per\n",
    "\n",
    "def run_all():\n",
    "    N, M = pick_sizes()\n",
    "    print(f\"Using SAME sizes on CPU and GPU for fairness: N={N}, M={M}\")\n",
    "    dev_cpu = torch.device(\"cpu\")\n",
    "    dev_gpu = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    res = {}\n",
    "\n",
    "    # CPU (float32)\n",
    "    per, gfl = bench_mm(dev_cpu, N, repeats=3, dtype=torch.float32)\n",
    "    res[\"CPU GEMM (fp32) s\"] = per; res[\"CPU GEMM (fp32) GF/s\"] = gfl\n",
    "    res[\"CPU Elem (fp32) s\"] = bench_elem(dev_cpu, N, repeats=2, dtype=torch.float32)\n",
    "    res[\"CPU FFT2 (c64) s\"] = bench_fft2(dev_cpu, M, repeats=2, complex_dtype=torch.complex64)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # GPU float32 (with TF32 enabled in backend)\n",
    "        per, gfl = bench_mm(dev_gpu, N, repeats=5, dtype=torch.float32)\n",
    "        res[\"GPU GEMM (fp32/TF32) s\"] = per; res[\"GPU GEMM (fp32/TF32) GF/s\"] = gfl\n",
    "        res[\"GPU Elem (fp32) s\"] = bench_elem(dev_gpu, N, repeats=3, dtype=torch.float32)\n",
    "        res[\"GPU FFT2 (c64) s\"] = bench_fft2(dev_gpu, M, repeats=3, complex_dtype=torch.complex64)\n",
    "\n",
    "        # GPU Tensor Core test (bfloat16 autocast)\n",
    "        per, gfl = bench_mm(dev_gpu, N, repeats=6, dtype=torch.float32, amp_dtype=torch.bfloat16)\n",
    "        res[\"GPU GEMM (bf16 autocast) s\"] = per; res[\"GPU GEMM (bf16) GF/s\"] = gfl\n",
    "\n",
    "        # Speedups\n",
    "        res[\"Speedup GEMM fp32\"] = res[\"CPU GEMM (fp32) s\"]/res[\"GPU GEMM (fp32/TF32) s\"]\n",
    "        res[\"Speedup GEMM bf16\"] = res[\"CPU GEMM (fp32) s\"]/res[\"GPU GEMM (bf16 autocast) s\"]\n",
    "        res[\"Speedup Elem fp32\"] = res[\"CPU Elem (fp32) s\"]/res[\"GPU Elem (fp32) s\"]\n",
    "        res[\"Speedup FFT2\"] = res[\"CPU FFT2 (c64) s\"]/res[\"GPU FFT2 (c64) s\"]\n",
    "\n",
    "    for k,v in res.items():\n",
    "        if isinstance(v, float):\n",
    "            print(f\"{k}: {v:.4f}\")\n",
    "        else:\n",
    "            print(f\"{k}: {v}\")\n",
    "    return res\n",
    "\n",
    "_ = run_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8931086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional: Tiny CNN throughput (images/sec) to demonstrate ML relevance.\n",
    "import torch, time\n",
    "\n",
    "def cnn_throughput(device, batch=128, steps=30, amp=False):\n",
    "    try:\n",
    "        from torch import nn\n",
    "        model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 7, stride=2, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64, 10),\n",
    "        ).to(device)\n",
    "    except Exception as e:\n",
    "        print(\"Model build failed:\", e); return\n",
    "\n",
    "    x = torch.randn(batch,3,224,224, device=device)\n",
    "    model.eval()\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        y = model(x)\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    if amp and device.type == \"cuda\":\n",
    "        for _ in range(steps):\n",
    "            with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                y = model(x)\n",
    "        torch.cuda.synchronize()\n",
    "    else:\n",
    "        for _ in range(steps):\n",
    "            y = model(x)\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "    elapsed = time.perf_counter()-t0\n",
    "    imgs = batch*steps/elapsed\n",
    "    print(f\"{'GPU' if device.type=='cuda' else 'CPU'} CNN throughput ({'bf16 amp' if amp else 'fp32'}): {imgs:.1f} images/s\")\n",
    "\n",
    "device_cpu = torch.device('cpu')\n",
    "cnn_throughput(device_cpu, batch=64, steps=20, amp=False)\n",
    "if torch.cuda.is_available():\n",
    "    device_gpu = torch.device('cuda')\n",
    "    cnn_throughput(device_gpu, batch=128, steps=40, amp=False)\n",
    "    cnn_throughput(device_gpu, batch=128, steps=40, amp=True)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
