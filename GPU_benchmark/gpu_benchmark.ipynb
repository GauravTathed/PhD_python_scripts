{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328891a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import subprocess, sys, time, math\n",
    "\n",
    "backend = None\n",
    "gpu_name = \"None\"\n",
    "has_cuda = False\n",
    "\n",
    "def run_cmd(x):\n",
    "    try:\n",
    "        p = subprocess.run(x, shell=True, capture_output=True, text=True, timeout=5)\n",
    "        if p.returncode == 0 and p.stdout.strip():\n",
    "            print(p.stdout.strip())\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    try:\n",
    "        import torch.backends.cuda\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    has_cuda = torch.cuda.is_available()\n",
    "    if has_cuda:\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        backend = \"torch\"\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if backend is None:\n",
    "    try:\n",
    "        import cupy as cp\n",
    "        gpu_name = cp.cuda.runtime.getDeviceProperties(0)[\"name\"].decode()\n",
    "        backend = \"cupy\"\n",
    "        has_cuda = True\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(f\"Backend: {backend}\")\n",
    "print(f\"GPU: {gpu_name}\")\n",
    "run_cmd(\"nvidia-smi -L\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db687b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time, math, sys\n",
    "\n",
    "def _fmt(x):\n",
    "    if x >= 1e3: return f\"{x/1e3:.2f}K\"\n",
    "    return f\"{x:.0f}\"\n",
    "\n",
    "def torch_bench():\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        free_mem, total_mem = torch.cuda.mem_get_info()\n",
    "        budget = int(min(free_mem, total_mem*0.5))\n",
    "        N = int((budget/12)**0.5)\n",
    "        N = max(2048, min(N, 8192))\n",
    "        N = (N//128)*128\n",
    "        M = min(4096, N)\n",
    "    else:\n",
    "        N = 2048\n",
    "        M = 2048\n",
    "\n",
    "    def bench_mm(device, N, repeats=5, dtype=torch.float32):\n",
    "        a = torch.randn((N,N), device=device, dtype=dtype)\n",
    "        b = torch.randn((N,N), device=device, dtype=dtype)\n",
    "        c = a @ b\n",
    "        if device.type == \"cuda\": torch.cuda.synchronize()\n",
    "        t0 = time.perf_counter()\n",
    "        for _ in range(repeats):\n",
    "            c = a @ b\n",
    "        if device.type == \"cuda\": torch.cuda.synchronize()\n",
    "        per = (time.perf_counter()-t0)/repeats\n",
    "        gflops = (2*N*N*N)/1e9/per\n",
    "        return per, gflops\n",
    "\n",
    "    def bench_elem(device, N, repeats=5):\n",
    "        x = torch.randn((N,N), device=device)\n",
    "        y = x\n",
    "        if device.type == \"cuda\": torch.cuda.synchronize()\n",
    "        t0 = time.perf_counter()\n",
    "        for _ in range(repeats):\n",
    "            y = torch.sin(y) + torch.exp(y) + torch.tanh(y)\n",
    "        if device.type == \"cuda\": torch.cuda.synchronize()\n",
    "        per = (time.perf_counter()-t0)/repeats\n",
    "        return per\n",
    "\n",
    "    def bench_fft2(device, M, repeats=3):\n",
    "        x = torch.randn((M,M), device=device, dtype=torch.complex64)\n",
    "        y = torch.fft.fft2(x)\n",
    "        if device.type == \"cuda\": torch.cuda.synchronize()\n",
    "        t0 = time.perf_counter()\n",
    "        for _ in range(repeats):\n",
    "            y = torch.fft.fft2(x)\n",
    "        if device.type == \"cuda\": torch.cuda.synchronize()\n",
    "        per = (time.perf_counter()-t0)/repeats\n",
    "        return per\n",
    "\n",
    "    dev_gpu = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dev_cpu = torch.device(\"cpu\")\n",
    "\n",
    "    N_cpu = min(N, 2048)\n",
    "    M_cpu = min(M, 2048)\n",
    "\n",
    "    res = {}\n",
    "    per, gflops = bench_mm(dev_cpu, N_cpu, repeats=3)\n",
    "    res[\"CPU GEMM s\"] = per\n",
    "    res[\"CPU GEMM GFLOP/s\"] = gflops\n",
    "    per = bench_elem(dev_cpu, N_cpu, repeats=2)\n",
    "    res[\"CPU Elem s\"] = per\n",
    "    per = bench_fft2(dev_cpu, M_cpu, repeats=2)\n",
    "    res[\"CPU FFT2 s\"] = per\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        per, gflops = bench_mm(dev_gpu, N, repeats=5)\n",
    "        res[\"GPU GEMM s\"] = per\n",
    "        res[\"GPU GEMM GFLOP/s\"] = gflops\n",
    "        per = bench_elem(dev_gpu, N, repeats=3)\n",
    "        res[\"GPU Elem s\"] = per\n",
    "        per = bench_fft2(dev_gpu, M, repeats=3)\n",
    "        res[\"GPU FFT2 s\"] = per\n",
    "        print(f\"N={N}, M={M}\")\n",
    "    else:\n",
    "        print(\"CUDA not available\")\n",
    "\n",
    "    keys = [\"CPU GEMM s\",\"GPU GEMM s\",\"CPU GEMM GFLOP/s\",\"GPU GEMM GFLOP/s\",\"CPU Elem s\",\"GPU Elem s\",\"CPU FFT2 s\",\"GPU FFT2 s\"]\n",
    "    for k in keys:\n",
    "        if k in res:\n",
    "            print(f\"{k}: {res[k]:.4f}\")\n",
    "    if \"GPU GEMM s\" in res:\n",
    "        print(f\"GEMM speedup: {res['CPU GEMM s']/res['GPU GEMM s']:.1f}x\")\n",
    "        print(f\"Elem speedup: {res['CPU Elem s']/res['GPU Elem s']:.1f}x\")\n",
    "        print(f\"FFT2 speedup: {res['CPU FFT2 s']/res['GPU FFT2 s']:.1f}x\")\n",
    "\n",
    "def cupy_bench():\n",
    "    import cupy as cp\n",
    "    free_mem, total_mem = cp.cuda.runtime.memGetInfo()\n",
    "    budget = int(min(free_mem, total_mem*0.5))\n",
    "    N = int((budget/12)**0.5)\n",
    "    N = max(2048, min(N, 8192))\n",
    "    N = (N//128)*128\n",
    "    M = min(4096, N)\n",
    "\n",
    "    def bench_mm(N, repeats=5):\n",
    "        a = cp.random.randn(N,N, dtype=cp.float32)\n",
    "        b = cp.random.randn(N,N, dtype=cp.float32)\n",
    "        c = a.dot(b)\n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "        t0 = time.perf_counter()\n",
    "        for _ in range(repeats):\n",
    "            c = a.dot(b)\n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "        per = (time.perf_counter()-t0)/repeats\n",
    "        gflops = (2*N*N*N)/1e9/per\n",
    "        return per, gflops\n",
    "\n",
    "    def bench_elem(N, repeats=5):\n",
    "        x = cp.random.randn(N,N).astype(cp.float32)\n",
    "        y = x\n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "        t0 = time.perf_counter()\n",
    "        for _ in range(repeats):\n",
    "            y = cp.sin(y) + cp.exp(y) + cp.tanh(y)\n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "        per = (time.perf_counter()-t0)/repeats\n",
    "        return per\n",
    "\n",
    "    def bench_fft2(M, repeats=3):\n",
    "        x = (cp.random.randn(M,M) + 1j*cp.random.randn(M,M)).astype(cp.complex64)\n",
    "        y = cp.fft.fft2(x)\n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "        t0 = time.perf_counter()\n",
    "        for _ in range(repeats):\n",
    "            y = cp.fft.fft2(x)\n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "        per = (time.perf_counter()-t0)/repeats\n",
    "        return per\n",
    "\n",
    "    per, gflops = bench_mm(N, repeats=5)\n",
    "    gpu_mm = per\n",
    "    gpu_gflops = gflops\n",
    "    per = bench_elem(N, repeats=3)\n",
    "    gpu_elem = per\n",
    "    per = bench_fft2(M, repeats=3)\n",
    "    gpu_fft = per\n",
    "    print(f\"N={N}, M={M}\")\n",
    "    print(f\"GPU GEMM s: {gpu_mm:.4f}\")\n",
    "    print(f\"GPU GEMM GFLOP/s: {gpu_gflops:.2f}\")\n",
    "    print(f\"GPU Elem s: {gpu_elem:.4f}\")\n",
    "    print(f\"GPU FFT2 s: {gpu_fft:.4f}\")\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    torch_bench()\n",
    "except Exception as e:\n",
    "    try:\n",
    "        import cupy as cp\n",
    "        cupy_bench()\n",
    "    except Exception as e2:\n",
    "        print(\"Install PyTorch (CUDA build) or CuPy to run the GPU benchmark.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
