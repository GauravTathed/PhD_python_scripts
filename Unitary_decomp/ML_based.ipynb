{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec07579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b15267d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, torch, torch.nn as nn, torch.nn.functional as F, math, time, random\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(0); np.random.seed(0); random.seed(0)\n",
    "dim=4\n",
    "allowed_edges = torch.tensor([[0,1],[0,2],[0,3]], dtype=torch.long, device=device)\n",
    "def op_with_phase(i,j,dim,phi):\n",
    "    M = torch.zeros(dim,dim, dtype=torch.complex64, device=device)\n",
    "    M[i,j] = torch.exp(1j*phi)\n",
    "    M[j,i] = torch.exp(-1j*phi)\n",
    "    return M\n",
    "def step_unitary(edge_w, frac, phase):\n",
    "    phi = math.pi*phase\n",
    "    H = torch.zeros(dim,dim, dtype=torch.complex64, device=device)\n",
    "    for e in range(allowed_edges.shape[0]):\n",
    "        i,j = allowed_edges[e]\n",
    "        H = H + edge_w[e]*op_with_phase(int(i),int(j),dim,phi)\n",
    "    theta = math.pi*frac\n",
    "    A = (-1j*0.5*theta)*H\n",
    "    return torch.linalg.matrix_exp(A)\n",
    "def unitary_from_seq(edges_idx, fracs, phases):\n",
    "    U = torch.eye(dim, dtype=torch.complex64, device=device)\n",
    "    for e, f, p in zip(edges_idx, fracs, phases):\n",
    "        w = torch.zeros(allowed_edges.shape[0], device=device); w[int(e)] = 1.0\n",
    "        U = step_unitary(w, f, p) @ U\n",
    "    return U\n",
    "def phase_align(U,V):\n",
    "    X = V.conj().transpose(-2,-1) @ U\n",
    "    tr = X.diagonal(dim1=-2,dim2=-1).sum(-1)\n",
    "    ang = torch.atan2(tr.imag, tr.real).view(-1,1,1)\n",
    "    return U * torch.exp(-1j*ang)\n",
    "\n",
    "def frob_loss(U,V):\n",
    "    d = U.shape[-1]\n",
    "    U2 = phase_align(U,V)\n",
    "    diff = U2 - V\n",
    "    sf = (diff.real**2 + diff.imag**2).sum(dim=(-2,-1))\n",
    "    return (sf/(d*d)).mean()\n",
    "\n",
    "def infidelity(U,V):\n",
    "    d = U.shape[-1]\n",
    "    X = V.conj().transpose(-2,-1) @ U\n",
    "    tr = X.diagonal(dim1=-2,dim2=-1).sum(-1).abs()\n",
    "    return (1.0 - tr/d).mean()\n",
    "def physics_loss(U_pred, U_true, fracs, w_frob=1.0, w_fid=1.0, w_len=0.1):\n",
    "    Lf = frob_loss(U_pred, U_true)\n",
    "    Li = infidelity(U_pred, U_true)\n",
    "    L1 = fracs.abs().sum()/fracs.numel()\n",
    "    return w_frob*Lf + w_fid*Li + w_len*L1, (Lf,Li,L1)\n",
    "def rand_seq(L, phase_set=(0.5,1.5), fmax=2.0):\n",
    "    edges = torch.randint(0,3,(L,), device=device)\n",
    "    fracs = fmax*torch.rand(L, device=device)\n",
    "    phases = torch.tensor(random.choices(phase_set,k=L), device=device)\n",
    "    return edges, fracs, phases\n",
    "\n",
    "def make_batch(B,L, fmax=2.0):\n",
    "    Us=[]; seqs=[]\n",
    "    for _ in range(B):\n",
    "        e,f,p = rand_seq(L, fmax=fmax)\n",
    "        U = unitary_from_seq(e,f,p)\n",
    "        Us.append(U.unsqueeze(0)); seqs.append((e,f,p))\n",
    "    U = torch.cat(Us,0)\n",
    "    return U, seqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c62fcd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_targets(seqs, L):\n",
    "    edges_true = torch.stack([s[0][:L] for s in seqs],0).long().to(device)\n",
    "    fracs_true = torch.stack([s[1][:L] for s in seqs],0).float().to(device)\n",
    "    phases_true = torch.stack([s[2][:L] for s in seqs],0).float().to(device)\n",
    "    return edges_true, fracs_true, phases_true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744c0e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "TRITON_OK = importlib.util.find_spec(\"triton\") is not None\n",
    "\n",
    "class InvNet(nn.Module):\n",
    "    def __init__(self, d=4, Lmax=5):\n",
    "        super().__init__()\n",
    "        inp = 2*d*d\n",
    "        h=1024\n",
    "        self.net = nn.Sequential(nn.Linear(inp,h), nn.GELU(), nn.Linear(h,h), nn.GELU(), nn.Linear(h,h), nn.GELU())\n",
    "        self.head_edges = nn.Linear(h, Lmax*3)\n",
    "        self.head_fracs = nn.Linear(h, Lmax)\n",
    "        self.head_phases = nn.Linear(h, Lmax)\n",
    "        self.Lmax=Lmax\n",
    "    def forward(self, x):\n",
    "        z = self.net(x)\n",
    "        logits = self.head_edges(z).view(-1,self.Lmax,3)\n",
    "        fracs = 2.0*torch.sigmoid(self.head_fracs(z))\n",
    "        phases = 2.0*torch.sigmoid(self.head_phases(z))\n",
    "        return logits, fracs, phases\n",
    "\n",
    "model = InvNet(d=dim, Lmax=5).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4, fused=(device=='cuda'))\n",
    "\n",
    "if TRITON_OK:\n",
    "    try:\n",
    "        model = torch.compile(model)\n",
    "        print(\"torch.compile enabled\")\n",
    "    except Exception:\n",
    "        print(\"torch.compile not enabled\")\n",
    "else:\n",
    "    print(\"No Triton detected; running without torch.compile\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9776cbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_unitary_batched(w, frac, phase):\n",
    "    B = w.shape[0]\n",
    "    ephi = torch.exp(1j*torch.pi*phase)\n",
    "    H = torch.zeros(B, dim, dim, dtype=torch.complex64, device=device)\n",
    "    for e in range(allowed_edges.shape[0]):\n",
    "        i, j = allowed_edges[e]\n",
    "        H[:, i, j] += w[:, e] * ephi\n",
    "        H[:, j, i] += w[:, e] * ephi.conj()\n",
    "    theta = torch.pi * frac\n",
    "    A = (-1j*0.5) * theta.view(-1,1,1) * H\n",
    "    return torch.linalg.matrix_exp(A)\n",
    "\n",
    "def pred_unitary_from_outputs(logits, fracs, phases, L, tau=1.0, hard=True):\n",
    "    B = logits.shape[0]\n",
    "    U = torch.eye(dim, dtype=torch.complex64, device=device).unsqueeze(0).repeat(B,1,1)\n",
    "    for t in range(L):\n",
    "        w = F.gumbel_softmax(logits[:, t, :], tau=tau, hard=hard)\n",
    "        f = fracs[:, t]\n",
    "        p = phases[:, t]\n",
    "        U_step = step_unitary_batched(w, f, p)\n",
    "        U = U_step @ U\n",
    "    return U\n",
    "\n",
    "\n",
    "def train_epoch(curr_L, steps=200, B=64, tau=1.0, w_frob=1.0, w_fid=1.0, w_len=0.1, use_sup=True, lam_edge=2.0, lam_frac=1.0, lam_phase=1.0, lam_ent=0.01, clip=1.0, fmax=2.0):\n",
    "    model.train()\n",
    "    mloss=0.0\n",
    "    for _ in range(steps):\n",
    "        U_true, seqs = make_batch(B, curr_L)\n",
    "        x = encode_unitary(U_true).to(device)\n",
    "        logits, fracs, phases = model(x)\n",
    "        U_pred = pred_unitary_from_outputs(logits, fracs, phases, curr_L, tau=tau, hard=True)\n",
    "        L,(Lf,Li,L1) = physics_loss(U_pred, U_true, fracs[:,:curr_L], w_frob, w_fid, w_len)\n",
    "        probs = logits[:,:curr_L,:].softmax(-1)\n",
    "        ent = -(probs.clamp_min(1e-8)*probs.clamp_min(1e-8).log()).sum(-1).mean()\n",
    "        L = L + lam_ent*ent\n",
    "        if use_sup:\n",
    "            e_t, f_t, p_t = pack_targets(seqs, curr_L)\n",
    "            ce = F.cross_entropy(logits[:,:curr_L,:].reshape(-1,3), e_t.reshape(-1))\n",
    "            mse_f = F.mse_loss(fracs[:,:curr_L], f_t)\n",
    "            mse_p = F.mse_loss(phases[:,:curr_L], p_t)\n",
    "            L = L + lam_edge*ce + lam_frac*mse_f + lam_phase*mse_p\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        L.backward()\n",
    "        if clip is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        opt.step()\n",
    "        mloss += L.item()\n",
    "    return mloss/steps\n",
    "\n",
    "def eval_epoch(curr_L, B=64, tau=0.5, w_frob=1.0, w_fid=1.0, w_len=0.1):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        U_true,_ = make_batch(B, curr_L)\n",
    "        x = encode_unitary(U_true)\n",
    "        logits, fracs, phases = model(x)\n",
    "        U_pred = pred_unitary_from_outputs(logits, fracs, phases, curr_L, tau=tau, hard=True)\n",
    "        L,(Lf,Li,L1) = physics_loss(U_pred, U_true, fracs[:,:curr_L], w_frob, w_fid, w_len)\n",
    "    return L.item(), Lf.item(), Li.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9665854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\iamga\\anaconda3\\Lib\\site-packages\\torch\\_inductor\\compile_fx.py:282: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TritonMissing",
     "evalue": "Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at: https://github.com/triton-lang/triton\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTritonMissing\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m tau \u001b[38;5;241m=\u001b[39m cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtau_start\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m (cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtau_end\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m-\u001b[39mcfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtau_start\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m*\u001b[39mt\n\u001b[0;32m      7\u001b[0m fmax \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.8\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1.2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m1.0\u001b[39m, (L\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4.0\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m tr \u001b[38;5;241m=\u001b[39m train_epoch(L, steps\u001b[38;5;241m=\u001b[39mcfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msteps_per_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m], B\u001b[38;5;241m=\u001b[39mcfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m], tau\u001b[38;5;241m=\u001b[39mtau, w_frob\u001b[38;5;241m=\u001b[39mcfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw_frob\u001b[39m\u001b[38;5;124m'\u001b[39m], w_fid\u001b[38;5;241m=\u001b[39mcfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw_fid\u001b[39m\u001b[38;5;124m'\u001b[39m], w_len\u001b[38;5;241m=\u001b[39mcfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw_len\u001b[39m\u001b[38;5;124m'\u001b[39m], use_sup\u001b[38;5;241m=\u001b[39mcfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_sup\u001b[39m\u001b[38;5;124m'\u001b[39m], lam_edge\u001b[38;5;241m=\u001b[39mcfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlam_edge\u001b[39m\u001b[38;5;124m'\u001b[39m], lam_frac\u001b[38;5;241m=\u001b[39mcfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlam_frac\u001b[39m\u001b[38;5;124m'\u001b[39m], lam_phase\u001b[38;5;241m=\u001b[39mcfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlam_phase\u001b[39m\u001b[38;5;124m'\u001b[39m], lam_ent\u001b[38;5;241m=\u001b[39mcfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlam_ent\u001b[39m\u001b[38;5;124m'\u001b[39m], clip\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, fmax\u001b[38;5;241m=\u001b[39mfmax)\n\u001b[0;32m      9\u001b[0m vl, vf, vi \u001b[38;5;241m=\u001b[39m eval_epoch(L, B\u001b[38;5;241m=\u001b[39mcfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m], tau\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m, w_frob\u001b[38;5;241m=\u001b[39mcfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw_frob\u001b[39m\u001b[38;5;124m'\u001b[39m], w_fid\u001b[38;5;241m=\u001b[39mcfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw_fid\u001b[39m\u001b[38;5;124m'\u001b[39m], w_len\u001b[38;5;241m=\u001b[39mcfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw_len\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     10\u001b[0m hist\u001b[38;5;241m.\u001b[39mappend((L, e, tr, vl, vf, vi))\n",
      "Cell \u001b[1;32mIn[32], line 31\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(curr_L, steps, B, tau, w_frob, w_fid, w_len, use_sup, lam_edge, lam_frac, lam_phase, lam_ent, clip, fmax)\u001b[0m\n\u001b[0;32m     29\u001b[0m U_true, seqs \u001b[38;5;241m=\u001b[39m make_batch(B, curr_L)\n\u001b[0;32m     30\u001b[0m x \u001b[38;5;241m=\u001b[39m encode_unitary(U_true)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 31\u001b[0m logits, fracs, phases \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[0;32m     32\u001b[0m U_pred \u001b[38;5;241m=\u001b[39m pred_unitary_from_outputs(logits, fracs, phases, curr_L, tau\u001b[38;5;241m=\u001b[39mtau, hard\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     33\u001b[0m L,(Lf,Li,L1) \u001b[38;5;241m=\u001b[39m physics_loss(U_pred, U_true, fracs[:,:curr_L], w_frob, w_fid, w_len)\n",
      "File \u001b[1;32mc:\\Users\\iamga\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:375\u001b[0m, in \u001b[0;36mOptimizedModule.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39m_has_any_global_hook():\n\u001b[0;32m    366\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    367\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `torch.compile(module)` when there are global hooks on \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodules (e.g., from `register_module_forward_hook`); this will\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    373\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    374\u001b[0m     )\n\u001b[1;32m--> 375\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\iamga\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\iamga\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\iamga\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:749\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>.compile_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    745\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__cause__\u001b[39;00m  \u001b[38;5;66;03m# User compiler error\u001b[39;00m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ShortenTraceback \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;66;03m# Failures in the backend likely don't have useful\u001b[39;00m\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;66;03m# data in the TorchDynamo frames, so we strip them out.\u001b[39;00m\n\u001b[1;32m--> 749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mremove_dynamo_frames() \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# see TORCHDYNAMO_VERBOSE=1\u001b[39;00m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    751\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[0;32m    752\u001b[0m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\iamga\\anaconda3\\Lib\\site-packages\\torch\\_inductor\\scheduler.py:4042\u001b[0m, in \u001b[0;36mScheduler.create_backend\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m   4040\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m GPUTooOldForTriton(device_props, inspect\u001b[38;5;241m.\u001b[39mcurrentframe())\n\u001b[0;32m   4041\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m is_gpu(device\u001b[38;5;241m.\u001b[39mtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 4042\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TritonMissing(inspect\u001b[38;5;241m.\u001b[39mcurrentframe())\n\u001b[0;32m   4044\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m device_scheduling(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mTritonMissing\u001b[0m: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at: https://github.com/triton-lang/triton\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"
     ]
    }
   ],
   "source": [
    "cfg = dict(w_frob=1.0, w_fid=1.0, w_len=0.02, epochs_per_L=6, total_L=5, steps_per_epoch=120, B=256, tau_start=0.8, tau_end=0.1, use_sup=True, lam_edge=2.0, lam_frac=1.0, lam_phase=1.0, lam_ent=0.02)\n",
    "hist = []\n",
    "for L in range(1, cfg['total_L']+1):\n",
    "    for e in range(cfg['epochs_per_L']):\n",
    "        t = 0 if cfg['epochs_per_L']==1 else e/(cfg['epochs_per_L']-1)\n",
    "        tau = cfg['tau_start'] + (cfg['tau_end']-cfg['tau_start'])*t\n",
    "        fmax = 0.8 + 1.2*min(1.0, (L-1)/4.0)\n",
    "        tr = train_epoch(L, steps=cfg['steps_per_epoch'], B=cfg['B'], tau=tau, w_frob=cfg['w_frob'], w_fid=cfg['w_fid'], w_len=cfg['w_len'], use_sup=cfg['use_sup'], lam_edge=cfg['lam_edge'], lam_frac=cfg['lam_frac'], lam_phase=cfg['lam_phase'], lam_ent=cfg['lam_ent'], clip=1.0, fmax=fmax)\n",
    "        vl, vf, vi = eval_epoch(L, B=cfg['B'], tau=0.05, w_frob=cfg['w_frob'], w_fid=cfg['w_fid'], w_len=cfg['w_len'])\n",
    "        hist.append((L, e, tr, vl, vf, vi))\n",
    "        print(f\"L={L} epoch={e} train={tr:.4f} val_total={vl:.4f} val_frob={vf:.4f} val_inf={vi:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80a61466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement triton (from versions: none)\n",
      "ERROR: No matching distribution found for triton\n"
     ]
    }
   ],
   "source": [
    "pip install triton\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26cdb06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
