{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f43ea0a1",
   "metadata": {},
   "source": [
    "# Fixed schedule "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "295a816b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, json, math, numpy as np, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def make_schedule(n, passes):\n",
    "    s=[]\n",
    "    for _ in range(passes):\n",
    "        for j in range(1,n):\n",
    "            s.append((0,j))\n",
    "    return s\n",
    "\n",
    "def givens_matrix(n,i,j,theta,phi):\n",
    "    G=np.eye(n,dtype=np.complex128)\n",
    "    c=np.cos(theta)\n",
    "    s=np.sin(theta)\n",
    "    e=np.exp(1j*phi)\n",
    "    G[i,i]=c\n",
    "    G[j,j]=c\n",
    "    G[i,j]=-e*s\n",
    "    G[j,i]=np.conj(e)*s\n",
    "    return G\n",
    "\n",
    "def unitary_from_params(n,schedule,theta,phi):\n",
    "    U=np.eye(n,dtype=np.complex128)\n",
    "    for k,(i,j) in enumerate(schedule):\n",
    "        U=givens_matrix(n,i,j,theta[k],phi[k])@U\n",
    "    return U\n",
    "\n",
    "def canon_phase(U):\n",
    "    d=np.linalg.det(U)\n",
    "    a=np.angle(d)/U.shape[0]\n",
    "    return U*np.exp(-1j*a)\n",
    "\n",
    "def vec_features(U):\n",
    "    return np.concatenate([U.real.reshape(-1),U.imag.reshape(-1)],0).astype(np.float32)\n",
    "\n",
    "def pack_labels(theta,phi):\n",
    "    cth=np.cos(theta); sth=np.sin(theta); cph=np.cos(phi); sph=np.sin(phi)\n",
    "    y=np.stack([cth,sth,cph,sph],-1).reshape(-1)\n",
    "    return y.astype(np.float32)\n",
    "\n",
    "def unpack_params(y):\n",
    "    y=y.reshape(-1,4)\n",
    "    u=y/np.clip(np.linalg.norm(y[:,:2],axis=1,keepdims=True),1e-9,None)\n",
    "    v=y/np.clip(np.linalg.norm(y[:,2:],axis=1,keepdims=True),1e-9,None)\n",
    "    cth=u[:,0]; sth=u[:,1]; cph=v[:,2]; sph=v[:,3]\n",
    "    theta=np.arctan2(sth,cth)\n",
    "    phi=np.arctan2(sph,cph)\n",
    "    theta=np.mod(theta,math.pi/2)\n",
    "    phi=np.mod(phi,2*math.pi)\n",
    "    return theta,phi\n",
    "\n",
    "class GenDataset(Dataset):\n",
    "    def __init__(self,n,passes,size,seed=0):\n",
    "        self.n=n; self.passes=passes; self.size=size\n",
    "        rng=np.random.default_rng(seed)\n",
    "        self.schedule=make_schedule(n,passes)\n",
    "        L=len(self.schedule)\n",
    "        self.theta=rng.uniform(0,math.pi/2,size=(size,L)).astype(np.float64)\n",
    "        self.phi=rng.uniform(0,2*math.pi,size=(size,L)).astype(np.float64)\n",
    "        X=[]; Y=[]\n",
    "        for i in range(size):\n",
    "            U=unitary_from_params(n,self.schedule,self.theta[i],self.phi[i])\n",
    "            U=canon_phase(U)\n",
    "            X.append(vec_features(U))\n",
    "            Y.append(pack_labels(self.theta[i],self.phi[i]))\n",
    "        self.X=np.stack(X,0)\n",
    "        self.Y=np.stack(Y,0)\n",
    "    def __len__(self): return self.size\n",
    "    def __getitem__(self,idx):\n",
    "        return torch.from_numpy(self.X[idx]), torch.from_numpy(self.Y[idx])\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,in_dim,out_dim,h=512):\n",
    "        super().__init__()\n",
    "        self.net=nn.Sequential(\n",
    "            nn.Linear(in_dim,h),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h,h),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h,out_dim)\n",
    "        )\n",
    "    def forward(self,x): return self.net(x)\n",
    "\n",
    "def train_model(n,passes,epochs,train_size,val_size,batch,lr,seed,device):\n",
    "    torch.manual_seed(seed); np.random.seed(seed)\n",
    "    schedule=make_schedule(n,passes)\n",
    "    L=len(schedule)\n",
    "    train_ds=GenDataset(n,passes,train_size,seed)\n",
    "    val_ds=GenDataset(n,passes,val_size,seed+1)\n",
    "    in_dim=2*n*n; out_dim=4*L\n",
    "    model=MLP(in_dim,out_dim).to(device)\n",
    "    opt=torch.optim.AdamW(model.parameters(),lr=lr)\n",
    "    best=float('inf'); best_state=None\n",
    "    train_loader=DataLoader(train_ds,batch_size=batch,shuffle=True,drop_last=True)\n",
    "    val_loader=DataLoader(val_ds,batch_size=batch,shuffle=False)\n",
    "    for ep in range(1,epochs+1):\n",
    "        model.train()\n",
    "        tot=0.0; cnt=0\n",
    "        for xb,yb in train_loader:\n",
    "            xb=xb.to(device); yb=yb.to(device)\n",
    "            pred=model(xb)\n",
    "            loss=F.mse_loss(pred,yb)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "            tot+=loss.item()*xb.size(0); cnt+=xb.size(0)\n",
    "        model.eval()\n",
    "        vtot=0.0; vcnt=0\n",
    "        with torch.no_grad():\n",
    "            for xb,yb in val_loader:\n",
    "                xb=xb.to(device); yb=yb.to(device)\n",
    "                pred=model(xb)\n",
    "                vtot+=F.mse_loss(pred,yb).item()*xb.size(0); vcnt+=xb.size(0)\n",
    "        vloss=vtot/vcnt\n",
    "        if vloss<best:\n",
    "            best=vloss\n",
    "            best_state={k:v.detach().cpu() for k,v in model.state_dict().items()}\n",
    "    model.load_state_dict(best_state)\n",
    "    meta={'n':n,'passes':passes,'schedule':schedule}\n",
    "    torch.save({'state_dict':model.state_dict(),'meta':meta},'givens_predictor.pt')\n",
    "    return model,meta\n",
    "\n",
    "def reconstruct_error(U,Uhat):\n",
    "    n=U.shape[0]\n",
    "    A=U.conj().T@Uhat\n",
    "    fid=np.abs(np.trace(A))/n\n",
    "    err=np.linalg.norm(U-Uhat,'fro')/np.linalg.norm(U,'fro')\n",
    "    return float(err),float(fid)\n",
    "\n",
    "def predict_params(model,meta,U):\n",
    "    n=meta['n']; schedule=meta['schedule']\n",
    "    U=canon_phase(U)\n",
    "    x=vec_features(U)\n",
    "    with torch.no_grad():\n",
    "        y=model(torch.from_numpy(x).unsqueeze(0)).cpu().numpy().reshape(-1)\n",
    "    theta,phi=unpack_params(y)\n",
    "    return theta,phi\n",
    "\n",
    "def synthesize(model_path,U,refine=False,steps=50):\n",
    "    ckpt=torch.load(model_path,map_location='cpu')\n",
    "    meta=ckpt['meta']\n",
    "    n=meta['n']; schedule=meta['schedule']\n",
    "    in_dim=2*n*n; out_dim=4*len(schedule)\n",
    "    model=MLP(in_dim,out_dim)\n",
    "    model.load_state_dict(ckpt['state_dict']); model.eval()\n",
    "    theta,phi=predict_params(model,meta,U)\n",
    "    Uhat=unitary_from_params(n,schedule,theta,phi)\n",
    "    if refine:\n",
    "        th=torch.tensor(theta,dtype=torch.float64,requires_grad=True)\n",
    "        ph=torch.tensor(phi,dtype=torch.float64,requires_grad=True)\n",
    "        target=torch.from_numpy(canon_phase(U))\n",
    "        opt=torch.optim.LBFGS([th,ph],max_iter=steps,history_size=10,line_search_fn='strong_wolfe')\n",
    "        def f():\n",
    "            opt.zero_grad()\n",
    "            Uh=torch.eye(n,dtype=torch.complex128)\n",
    "            for k,(i,j) in enumerate(schedule):\n",
    "                c=torch.cos(th[k]); s=torch.sin(th[k]); e=torch.exp(1j*ph[k])\n",
    "                G=torch.eye(n,dtype=torch.complex128)\n",
    "                G[i,i]=c; G[j,j]=c; G[i,j]=-e*s; G[j,i]=torch.conj(e)*s\n",
    "                Uh=G@Uh\n",
    "            L=torch.norm(target-Uh)**2\n",
    "            L.backward()\n",
    "            return L\n",
    "        opt.step(f)\n",
    "        theta=th.detach().numpy(); phi=ph.detach().numpy()\n",
    "        Uhat=unitary_from_params(n,schedule,theta,phi)\n",
    "    e,f=reconstruct_error(canon_phase(U),canon_phase(Uhat))\n",
    "    seq=[{'pair':schedule[k],'theta':float(theta[k]),'phi':float(phi[k])} for k in range(len(schedule))]\n",
    "    return {'error_fro':e,'fidelity':f,'sequence':seq,'n':n,'passes':meta['passes']}\n",
    "\n",
    "p=argparse.ArgumentParser()\n",
    "p.add_argument('--n',type=int,default=5)\n",
    "p.add_argument('--passes',type=int,default=3)\n",
    "p.add_argument('--epochs',type=int,default=20)\n",
    "p.add_argument('--train_size',type=int,default=20000)\n",
    "p.add_argument('--val_size',type=int,default=2000)\n",
    "p.add_argument('--batch',type=int,default=256)\n",
    "p.add_argument('--lr',type=float,default=1e-3)\n",
    "p.add_argument('--seed',type=int,default=0)\n",
    "p.add_argument('--device',type=str,default='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "p.add_argument('--demo',action='store_true')\n",
    "p.add_argument('--refine',action='store_true')\n",
    "args, _ = p.parse_known_args()\n",
    "model,meta=train_model(args.n,args.passes,args.epochs,args.train_size,args.val_size,args.batch,args.lr,args.seed,args.device)\n",
    "if args.demo:\n",
    "    rng=np.random.default_rng(1)\n",
    "    schedule=meta['schedule']\n",
    "    L=len(schedule)\n",
    "    theta=rng.uniform(0,math.pi/2,size=L)\n",
    "    phi=rng.uniform(0,2*math.pi,size=L)\n",
    "    U=unitary_from_params(args.n,schedule,theta,phi)\n",
    "    res=synthesize('givens_predictor.pt',U,refine=args.refine)\n",
    "    print(json.dumps(res,indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e334f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, meta = train_model(\n",
    "    n=8, passes=2, epochs=10,\n",
    "    train_size=40000, val_size=4000,\n",
    "    batch=128, lr=1e-2, seed=0, device='cuda'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17ea4d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta =  [0.804  1.493  0.2264 1.4901 0.4898 0.665  1.3002 0.6428 0.8633 0.0433\n",
      " 1.1836 0.8453 0.5179 1.2385 0.4763 0.7124] \n",
      " phi =  [0.8422 2.5328 1.2783 1.6482 4.7147 1.7619 3.0485 6.1622 6.0423 4.554\n",
      " 3.4006 1.7398 1.0094 6.0942 3.2426 0.728 ]\n",
      "{\n",
      "  \"error_fro\": 0.04651983384421423,\n",
      "  \"fidelity\": 0.9989179525295542\n",
      "}\n",
      "[{'pair': (0, 1), 'theta': 0.806848958979458, 'phi': 0.8649081953053227}, {'pair': (0, 2), 'theta': 1.419492420185325, 'phi': 2.564695963199446}, {'pair': (0, 3), 'theta': 0.21704903974166365, 'phi': 1.295170649341052}, {'pair': (0, 4), 'theta': 1.5683619014019317, 'phi': 5.058463348204238}, {'pair': (0, 5), 'theta': -0.5056356922701988, 'phi': 5.200416863077462}, {'pair': (0, 6), 'theta': 0.6251958217001425, 'phi': -1.0993202009371799}, {'pair': (0, 7), 'theta': 1.3065092052472647, 'phi': 0.3866110239607623}, {'pair': (0, 1), 'theta': -0.6424136480184369, 'phi': 0.32946390263442515}, {'pair': (0, 2), 'theta': -0.8561109229702702, 'phi': 0.20672729648353366}, {'pair': (0, 3), 'theta': -0.041397420166249116, 'phi': 5.663396752944311}, {'pair': (0, 4), 'theta': 1.9568108867752294, 'phi': 0.7295425467748805}, {'pair': (0, 5), 'theta': -0.8365906572275249, 'phi': -1.1288382742966987}, {'pair': (0, 6), 'theta': 0.5608401287927836, 'phi': 1.1531843976766905}, {'pair': (0, 7), 'theta': 1.1949679905899344, 'phi': 6.307137610942827}]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, math, json\n",
    "rng = np.random.default_rng(1)\n",
    "L = 16#len(meta['schedule'])\n",
    "theta_true = rng.uniform(0, math.pi/2, size=L)\n",
    "phi_true = rng.uniform(0, 2*math.pi, size=L)\n",
    "print('theta = ',theta_true, '\\n phi = ', phi_true)\n",
    "U = unitary_from_params(meta['n'], meta['schedule'], theta_true, phi_true)\n",
    "res = synthesize('givens_predictor.pt', U, refine=True)\n",
    "# print(res)\n",
    "print(json.dumps({k: res[k] for k in ['error_fro','fidelity']}, indent=2))\n",
    "print(res['sequence'][:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e40c5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U_true_raw:\n",
      " [[ 0.028 +0.0209j -0.0427-0.0313j -0.0295-0.1818j -0.0415-0.0044j\n",
      "   0.0812-0.6616j  0.031 +0.1206j -0.0131-0.6731j -0.2128+0.0442j]\n",
      " [ 0.3842-0.43j    0.5551-0.0004j  0.0077-0.0041j -0.0003-0.002j\n",
      "   0.0219-0.1087j -0.0073+0.0589j  0.0304-0.0941j  0.5774+0.0162j]\n",
      " [-0.6145-0.0401j -0.2583+0.425j   0.0588-0.0032j -0.0001-0.002j\n",
      "   0.0352-0.1069j -0.0145+0.0584j  0.042 -0.0911j  0.5798+0.0864j]\n",
      " [-0.0167-0.0003j  0.0122-0.0111j -0.0696-0.2148j  0.9736-0.j\n",
      "   0.0041+0.001j  -0.0022-0.0004j  0.0035+0.0013j -0.0014+0.0216j]\n",
      " [-0.3972-0.3163j  0.5119+0.3335j -0.1949-0.2977j -0.0938-0.0038j\n",
      "   0.0465+0.0876j -0.0121-0.0461j  0.0054+0.0792j -0.4353+0.1599j]\n",
      " [ 0.0647-0.1112j -0.0855+0.1786j -0.5377+0.4082j  0.059 +0.1438j\n",
      "   0.2882-0.0298j  0.5998-0.0024j -0.0242-0.0005j -0.0366-0.1367j]\n",
      " [ 0.0542-0.0204j -0.0829+0.035j  -0.3191-0.074j  -0.0355+0.0659j\n",
      "  -0.5805-0.0956j  0.0096-0.262j   0.676 -0.0073j  0.028 -0.0554j]\n",
      " [ 0.0596+0.0738j -0.0992-0.1049j -0.1248-0.4543j -0.1083+0.0012j\n",
      "   0.2445+0.176j   0.1267-0.7284j -0.1818-0.1732j  0.1898+0.0099j]]\n",
      "U_pred_raw:\n",
      " [[ 0.0168+0.0218j -0.0361-0.0425j -0.0451-0.183j  -0.0437-0.0023j\n",
      "   0.0764-0.6655j  0.0187+0.1202j -0.0043-0.6727j -0.2062+0.0091j]\n",
      " [ 0.375 -0.44j    0.5539+0.j     -0.0002+0.0002j  0.    +0.j\n",
      "   0.0018-0.111j  -0.0097+0.0607j  0.013 -0.0907j  0.5774+0.033j ]\n",
      " [-0.6165-0.0236j -0.2505+0.4257j  0.0986+0.0002j  0.    +0.0001j\n",
      "   0.0156-0.111j  -0.0172+0.0596j  0.0242-0.0892j  0.5743+0.1044j]\n",
      " [-0.002 -0.0015j -0.0135-0.0165j -0.0663-0.2066j  0.9757+0.j\n",
      "   0.0033-0.0023j -0.002 +0.001j   0.0029-0.0015j  0.0112+0.0177j]\n",
      " [-0.4121-0.3126j  0.5146+0.3345j -0.2066-0.2715j -0.0577+0.0107j\n",
      "   0.0328+0.0835j -0.0119-0.0484j  0.019 +0.0718j -0.4415+0.1576j]\n",
      " [ 0.0442-0.0878j -0.0819+0.1818j -0.5271+0.4222j  0.0583+0.1455j\n",
      "   0.2938-0.0417j  0.6012+0.0007j -0.0224-0.0007j -0.0078-0.1412j]\n",
      " [ 0.0458-0.0113j -0.0917+0.0271j -0.3176-0.0581j -0.0343+0.0667j\n",
      "  -0.5844-0.0802j  0.019 -0.252j   0.6799-0.0084j  0.0489-0.0471j]\n",
      " [ 0.0442+0.054j  -0.0945-0.1056j -0.1228-0.4646j -0.1115-0.004j\n",
      "   0.2286+0.1732j  0.1591-0.7242j -0.1821-0.1705j  0.1899+0.0357j]]\n",
      "U_true_canon:\n",
      " [[ 0.028 +0.0209j -0.0427-0.0313j -0.0295-0.1818j -0.0415-0.0044j\n",
      "   0.0812-0.6616j  0.031 +0.1206j -0.0131-0.6731j -0.2128+0.0442j]\n",
      " [ 0.3842-0.43j    0.5551-0.0004j  0.0077-0.0041j -0.0003-0.002j\n",
      "   0.0219-0.1087j -0.0073+0.0589j  0.0304-0.0941j  0.5774+0.0162j]\n",
      " [-0.6145-0.0401j -0.2583+0.425j   0.0588-0.0032j -0.0001-0.002j\n",
      "   0.0352-0.1069j -0.0145+0.0584j  0.042 -0.0911j  0.5798+0.0864j]\n",
      " [-0.0167-0.0003j  0.0122-0.0111j -0.0696-0.2148j  0.9736-0.j\n",
      "   0.0041+0.001j  -0.0022-0.0004j  0.0035+0.0013j -0.0014+0.0216j]\n",
      " [-0.3972-0.3163j  0.5119+0.3335j -0.1949-0.2977j -0.0938-0.0038j\n",
      "   0.0465+0.0876j -0.0121-0.0461j  0.0054+0.0792j -0.4353+0.1599j]\n",
      " [ 0.0647-0.1112j -0.0855+0.1786j -0.5377+0.4082j  0.059 +0.1438j\n",
      "   0.2882-0.0298j  0.5998-0.0024j -0.0242-0.0005j -0.0366-0.1367j]\n",
      " [ 0.0542-0.0204j -0.0829+0.035j  -0.3191-0.074j  -0.0355+0.0659j\n",
      "  -0.5805-0.0956j  0.0096-0.262j   0.676 -0.0073j  0.028 -0.0554j]\n",
      " [ 0.0596+0.0738j -0.0992-0.1049j -0.1248-0.4543j -0.1083+0.0012j\n",
      "   0.2445+0.176j   0.1267-0.7284j -0.1818-0.1732j  0.1898+0.0099j]]\n",
      "U_pred_canon:\n",
      " [[ 0.0168+0.0218j -0.0361-0.0425j -0.0451-0.183j  -0.0437-0.0023j\n",
      "   0.0764-0.6655j  0.0187+0.1202j -0.0043-0.6727j -0.2062+0.0091j]\n",
      " [ 0.375 -0.44j    0.5539+0.j     -0.0002+0.0002j  0.    +0.j\n",
      "   0.0018-0.111j  -0.0097+0.0607j  0.013 -0.0907j  0.5774+0.033j ]\n",
      " [-0.6165-0.0236j -0.2505+0.4257j  0.0986+0.0002j  0.    +0.0001j\n",
      "   0.0156-0.111j  -0.0172+0.0596j  0.0242-0.0892j  0.5743+0.1044j]\n",
      " [-0.002 -0.0015j -0.0135-0.0165j -0.0663-0.2066j  0.9757+0.j\n",
      "   0.0033-0.0023j -0.002 +0.001j   0.0029-0.0015j  0.0112+0.0177j]\n",
      " [-0.4121-0.3126j  0.5146+0.3345j -0.2066-0.2715j -0.0577+0.0107j\n",
      "   0.0328+0.0835j -0.0119-0.0484j  0.019 +0.0718j -0.4415+0.1576j]\n",
      " [ 0.0442-0.0878j -0.0819+0.1818j -0.5271+0.4222j  0.0583+0.1455j\n",
      "   0.2938-0.0417j  0.6012+0.0007j -0.0224-0.0007j -0.0078-0.1412j]\n",
      " [ 0.0458-0.0113j -0.0917+0.0271j -0.3176-0.0581j -0.0343+0.0667j\n",
      "  -0.5844-0.0802j  0.019 -0.252j   0.6799-0.0084j  0.0489-0.0471j]\n",
      " [ 0.0442+0.054j  -0.0945-0.1056j -0.1228-0.4646j -0.1115-0.004j\n",
      "   0.2286+0.1732j  0.1591-0.7242j -0.1821-0.1705j  0.1899+0.0357j]]\n",
      "U_true_canon - U_pred_canon:\n",
      " [[ 0.0112-0.0009j -0.0066+0.0112j  0.0155+0.0012j  0.0022-0.0021j\n",
      "   0.0049+0.0039j  0.0124+0.0004j -0.0088-0.0004j -0.0066+0.035j ]\n",
      " [ 0.0092+0.01j    0.0012-0.0004j  0.0079-0.0043j -0.0004-0.002j\n",
      "   0.02  +0.0023j  0.0024-0.0018j  0.0174-0.0034j -0.    -0.0169j]\n",
      " [ 0.002 -0.0165j -0.0078-0.0006j -0.0398-0.0034j -0.0001-0.0021j\n",
      "   0.0197+0.0041j  0.0027-0.0012j  0.0178-0.0019j  0.0055-0.0181j]\n",
      " [-0.0147+0.0012j  0.0257+0.0053j -0.0033-0.0082j -0.0021-0.j\n",
      "   0.0007+0.0033j -0.0002-0.0014j  0.0005+0.0028j -0.0126+0.0039j]\n",
      " [ 0.0149-0.0037j -0.0027-0.001j   0.0117-0.0262j -0.036 -0.0144j\n",
      "   0.0138+0.0041j -0.0002+0.0023j -0.0136+0.0074j  0.0062+0.0023j]\n",
      " [ 0.0205-0.0234j -0.0036-0.0032j -0.0106-0.014j   0.0007-0.0018j\n",
      "  -0.0055+0.0119j -0.0014-0.0031j -0.0018+0.0001j -0.0288+0.0046j]\n",
      " [ 0.0084-0.0091j  0.0087+0.008j  -0.0014-0.0159j -0.0012-0.0008j\n",
      "   0.0039-0.0154j -0.0093-0.0099j -0.0039+0.0011j -0.0209-0.0083j]\n",
      " [ 0.0153+0.0198j -0.0047+0.0007j -0.002 +0.0104j  0.0032+0.0052j\n",
      "   0.0159+0.0027j -0.0324-0.0042j  0.0003-0.0028j -0.0001-0.0258j]]\n",
      "fro_error_canon = 0.04651983384421423\n",
      "fidelity_canon  = 0.9989179525295542\n"
     ]
    }
   ],
   "source": [
    "theta_pred = np.array([s['theta'] for s in res['sequence']])\n",
    "phi_pred = np.array([s['phi'] for s in res['sequence']])\n",
    "\n",
    "U_true_raw = unitary_from_params(meta['n'], meta['schedule'], theta_true, phi_true)\n",
    "U_pred_raw = unitary_from_params(meta['n'], meta['schedule'], theta_pred, phi_pred)\n",
    "\n",
    "U_true = canon_phase(U_true_raw)\n",
    "U_pred = canon_phase(U_pred_raw)\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print(\"U_true_raw:\\n\", U_true_raw)\n",
    "print(\"U_pred_raw:\\n\", U_pred_raw)\n",
    "print(\"U_true_canon:\\n\", U_true)\n",
    "print(\"U_pred_canon:\\n\", U_pred)\n",
    "print(\"U_true_canon - U_pred_canon:\\n\", U_true - U_pred)\n",
    "\n",
    "err = np.linalg.norm(U_true - U_pred, 'fro')/np.linalg.norm(U_true, 'fro')\n",
    "fid = np.abs(np.trace(U_true.conj().T @ U_pred))/U_true.shape[0]\n",
    "print(\"fro_error_canon =\", float(err))\n",
    "print(\"fidelity_canon  =\", float(fid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d5a78e",
   "metadata": {},
   "source": [
    "# Predicting a single rotation with angles and phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "105737f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, json, numpy as np, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def givens_matrix(n,i,j,theta,phi):\n",
    "    G=np.eye(n,dtype=np.complex128)\n",
    "    c=np.cos(theta); s=np.sin(theta); e=np.exp(1j*phi)\n",
    "    G[i,i]=c; G[j,j]=c; G[i,j]=-e*s; G[j,i]=np.conj(e)*s\n",
    "    return G\n",
    "\n",
    "def unitary_from_pair(n,pair,theta,phi):\n",
    "    return givens_matrix(n,pair[0],pair[1],theta,phi)\n",
    "\n",
    "def canon_phase(U):\n",
    "    a=np.angle(np.linalg.det(U))/U.shape[0]\n",
    "    return U*np.exp(-1j*a)\n",
    "\n",
    "def vec_features(U):\n",
    "    return np.concatenate([U.real.reshape(-1),U.imag.reshape(-1)],0).astype(np.float32)\n",
    "\n",
    "def pack_angles(theta,phi):\n",
    "    return np.array([np.cos(theta),np.sin(theta),np.cos(phi),np.sin(phi)],dtype=np.float32)\n",
    "\n",
    "def unpack_angles(y):\n",
    "    cth,sth,cph,sph=y\n",
    "    theta=np.arctan2(sth,cth)\n",
    "    phi=np.arctan2(sph,cph)\n",
    "    theta=np.mod(theta,math.pi/2)\n",
    "    phi=np.mod(phi,2*math.pi)\n",
    "    return theta,phi\n",
    "\n",
    "def fidelity(U,Uhat):\n",
    "    n=U.shape[0]\n",
    "    return float(np.abs(np.trace(U.conj().T@Uhat))/n)\n",
    "\n",
    "class PairAngleDataset(Dataset):\n",
    "    def __init__(self,n,pairs,size,seed=0):\n",
    "        self.n=n; self.pairs=pairs; self.size=size\n",
    "        rng=np.random.default_rng(seed)\n",
    "        X=[]; y_pair=[]; y_ang=[]\n",
    "        for _ in range(size):\n",
    "            k=rng.integers(0,len(pairs))\n",
    "            th=rng.uniform(0,math.pi/2); ph=rng.uniform(0,2*math.pi)\n",
    "            U=unitary_from_pair(n,pairs[k],th,ph)\n",
    "            U=canon_phase(U)\n",
    "            X.append(vec_features(U))\n",
    "            y_pair.append(k)\n",
    "            y_ang.append(pack_angles(th,ph))\n",
    "        self.X=np.stack(X,0); self.y_pair=np.array(y_pair,dtype=np.int64); self.y_ang=np.stack(y_ang,0)\n",
    "    def __len__(self): return self.size\n",
    "    def __getitem__(self,idx):\n",
    "        return torch.from_numpy(self.X[idx]), torch.tensor(self.y_pair[idx]), torch.from_numpy(self.y_ang[idx])\n",
    "\n",
    "class Trunk(nn.Module):\n",
    "    def __init__(self,in_dim,h=512):\n",
    "        super().__init__()\n",
    "        self.net=nn.Sequential(nn.Linear(in_dim,h),nn.ReLU(),nn.Linear(h,h),nn.ReLU())\n",
    "    def forward(self,x): return self.net(x)\n",
    "\n",
    "class PairAngleNet(nn.Module):\n",
    "    def __init__(self,in_dim,num_pairs,h=512):\n",
    "        super().__init__()\n",
    "        self.trunk=Trunk(in_dim,h)\n",
    "        self.head_pair=nn.Linear(h,num_pairs)\n",
    "        self.head_angles=nn.Linear(h,num_pairs*4)\n",
    "        self.num_pairs=num_pairs\n",
    "    def forward(self,x):\n",
    "        z=self.trunk(x)\n",
    "        logits=self.head_pair(z)\n",
    "        ang=self.head_angles(z).view(-1,self.num_pairs,4)\n",
    "        return logits, ang\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_fid(model,loader,n,pairs,device):\n",
    "    model.eval()\n",
    "    N=0; s=0.0; ltot=0.0\n",
    "    for xb,yb_pair,yb_ang in loader:\n",
    "        xb=xb.to(device); yb_pair=yb_pair.to(device); yb_ang=yb_ang.to(device)\n",
    "        logits, angs=model(xb)\n",
    "        ce=F.cross_entropy(logits,yb_pair)\n",
    "        idx=yb_pair.view(-1,1,1).expand(-1,1,4)\n",
    "        sel=torch.gather(angs,1,idx).squeeze(1)\n",
    "        mse=F.mse_loss(sel,yb_ang)\n",
    "        l=ce+mse\n",
    "        preds=logits.argmax(dim=1)\n",
    "        sel_pred=torch.gather(angs,1,preds.view(-1,1,1).expand(-1,1,4)).squeeze(1).cpu().numpy()\n",
    "        y_true=yb_ang.cpu().numpy()\n",
    "        pairs_pred=preds.cpu().numpy()\n",
    "        for b in range(xb.size(0)):\n",
    "            theta_t,phi_t=unpack_angles(y_true[b])\n",
    "            theta_p,phi_p=unpack_angles(sel_pred[b])\n",
    "            U_t=canon_phase(unitary_from_pair(n,pairs[yb_pair[b].item()],theta_t,phi_t))\n",
    "            U_p=canon_phase(unitary_from_pair(n,pairs[pairs_pred[b]],theta_p,phi_p))\n",
    "            s+=fidelity(U_t,U_p); N+=1\n",
    "        ltot+=l.item()*xb.size(0)\n",
    "    return ltot/max(1,N), s/max(1,N)\n",
    "\n",
    "def train_pair_selector(n=4,pairs=[(0,1),(0,2),(0,3)],epochs=15,train_size=16000,val_size=2000,batch=256,lr=1e-3,seed=0,device='cpu'):\n",
    "    torch.manual_seed(seed); np.random.seed(seed)\n",
    "    train_ds=PairAngleDataset(n,pairs,train_size,seed)\n",
    "    val_ds=PairAngleDataset(n,pairs,val_size,seed+1)\n",
    "    in_dim=2*n*n; P=len(pairs)\n",
    "    model=PairAngleNet(in_dim,P,512).to(device)\n",
    "    opt=torch.optim.AdamW(model.parameters(),lr=lr)\n",
    "    train_loader=DataLoader(train_ds,batch_size=batch,shuffle=True,drop_last=True)\n",
    "    val_loader=DataLoader(val_ds,batch_size=batch,shuffle=False)\n",
    "    best_fid=-1.0; best_state=None\n",
    "    for ep in range(1,epochs+1):\n",
    "        model.train()\n",
    "        tot=0.0; cnt=0\n",
    "        for xb,yb_pair,yb_ang in train_loader:\n",
    "            xb=xb.to(device); yb_pair=yb_pair.to(device); yb_ang=yb_ang.to(device)\n",
    "            logits, angs=model(xb)\n",
    "            ce=F.cross_entropy(logits,yb_pair)\n",
    "            idx=yb_pair.view(-1,1,1).expand(-1,1,4)\n",
    "            sel=torch.gather(angs,1,idx).squeeze(1)\n",
    "            mse=F.mse_loss(sel,yb_ang)\n",
    "            loss=ce+mse\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "            tot+=loss.item()*xb.size(0); cnt+=xb.size(0)\n",
    "        train_loss=tot/cnt\n",
    "        vloss,vfid=eval_fid(model,val_loader,n,pairs,device)\n",
    "        tloss,tfid=eval_fid(model,DataLoader(train_ds,batch_size=batch,shuffle=False),n,pairs,device)\n",
    "        if vfid>best_fid:\n",
    "            best_fid=vfid\n",
    "            best_state={k:v.detach().cpu() for k,v in model.state_dict().items()}\n",
    "        print(f\"epoch {ep:02d} | train_loss {train_loss:.5f} val_loss {vloss:.5f} | train_fid {tfid:.6f} val_fid {vfid:.6f}\")\n",
    "    model.load_state_dict(best_state)\n",
    "    meta={'n':n,'pairs':pairs}\n",
    "    torch.save({'state_dict':model.state_dict(),'meta':meta},'pair_angle_selector.pt')\n",
    "    return model,meta\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_one(model,meta,U):\n",
    "    n=meta['n']; pairs=meta['pairs']\n",
    "    x=torch.from_numpy(vec_features(canon_phase(U))).unsqueeze(0).float()\n",
    "    logits, angs=model(x)\n",
    "    k=int(torch.argmax(logits,dim=1).item())\n",
    "    y=angs[0,k].numpy()\n",
    "    th,ph=unpack_angles(y)\n",
    "    Uhat=canon_phase(unitary_from_pair(n,pairs[k],th,ph))\n",
    "    return {'pair':pairs[k],'theta':float(th),'phi':float(ph),'U_pred':Uhat}\n",
    "\n",
    "def demo_once(n=4,pairs=[(0,1),(0,2),(0,3)],seed=123):\n",
    "    rng=np.random.default_rng(seed)\n",
    "    k=int(rng.integers(0,len(pairs)))\n",
    "    th=rng.uniform(0,math.pi/2); ph=rng.uniform(0,2*math.pi)\n",
    "    U=canon_phase(unitary_from_pair(n,pairs[k],th,ph))\n",
    "    ckpt=torch.load('pair_angle_selector.pt',map_location='cpu')\n",
    "    in_dim=2*n*n; P=len(pairs)\n",
    "    model=PairAngleNet(in_dim,P,512)\n",
    "    model.load_state_dict(ckpt['state_dict']); model.eval()\n",
    "    pred=predict_one(model,{'n':n,'pairs':pairs},U)\n",
    "    fid_val=fidelity(U,pred['U_pred'])\n",
    "    return {'true_pair':pairs[k],'true_theta':float(th),'true_phi':float(ph),'pred_pair':pred['pair'],'pred_theta':pred['theta'],'pred_phi':pred['phi'],'fidelity':fid_val,'U_true':U,'U_pred':pred['U_pred']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b2b446fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01 | train_loss 1.13806 val_loss 0.74089 | train_fid 0.984408 val_fid 0.984212\n",
      "epoch 02 | train_loss 0.50484 val_loss 0.34289 | train_fid 0.998844 val_fid 0.998802\n",
      "epoch 03 | train_loss 0.27286 val_loss 0.21350 | train_fid 0.999095 val_fid 0.999057\n",
      "epoch 04 | train_loss 0.18329 val_loss 0.15492 | train_fid 0.999263 val_fid 0.999234\n",
      "epoch 05 | train_loss 0.13798 val_loss 0.12163 | train_fid 0.999576 val_fid 0.999568\n"
     ]
    }
   ],
   "source": [
    "model, meta = train_pair_selector(n=4, pairs=[(0,1),(0,2),(0,3)], epochs=5, train_size=30000, val_size=3000, batch=256, lr=1e-4, seed=0, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "344bdc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"true_pair\": [\n",
      "    0,\n",
      "    3\n",
      "  ],\n",
      "  \"true_theta\": 0.37198107390759205,\n",
      "  \"true_phi\": 5.034555946803014,\n",
      "  \"pred_pair\": [\n",
      "    0,\n",
      "    3\n",
      "  ],\n",
      "  \"pred_theta\": 0.3677932918071747,\n",
      "  \"pred_phi\": 5.039814472198486,\n",
      "  \"fidelity\": 0.9999947154523011\n",
      "}\n",
      "U_true:\n",
      " [[ 0.9316+0.j      0.    +0.j      0.    +0.j     -0.1151+0.3448j]\n",
      " [ 0.    +0.j      1.    +0.j      0.    +0.j      0.    +0.j    ]\n",
      " [ 0.    +0.j      0.    +0.j      1.    +0.j      0.    +0.j    ]\n",
      " [ 0.1151+0.3448j  0.    +0.j      0.    +0.j      0.9316+0.j    ]]\n",
      "U_pred:\n",
      " [[ 0.9331-0.j      0.    +0.j      0.    +0.j     -0.1156+0.3405j]\n",
      " [ 0.    +0.j      1.    -0.j      0.    +0.j      0.    +0.j    ]\n",
      " [ 0.    +0.j      0.    +0.j      1.    -0.j      0.    +0.j    ]\n",
      " [ 0.1156+0.3405j  0.    +0.j      0.    +0.j      0.9331-0.j    ]]\n"
     ]
    }
   ],
   "source": [
    "res = demo_once(n=4, pairs=[(0,1),(0,2),(0,3)], seed=3)\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "print(json.dumps({k:res[k] for k in ['true_pair','true_theta','true_phi','pred_pair','pred_theta','pred_phi','fidelity']}, indent=2))\n",
    "print('U_true:\\n', res['U_true'])\n",
    "print('U_pred:\\n', res['U_pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997a41e8",
   "metadata": {},
   "source": [
    "# Predicts single or double rotations (theta restricted from 0 to pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff7c96d",
   "metadata": {},
   "source": [
    "## Two rotatiosn fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "72e80bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, json, numpy as np, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def givens_matrix(n,i,j,theta,phi):\n",
    "    G=np.eye(n,dtype=np.complex128)\n",
    "    c=np.cos(theta); s=np.sin(theta); e=np.exp(1j*phi)\n",
    "    G[i,i]=c; G[j,j]=c; G[i,j]=-e*s; G[j,i]=np.conj(e)*s\n",
    "    return G\n",
    "\n",
    "def apply_two(n,p1,th1,ph1,p2,th2,ph2):\n",
    "    U=np.eye(n,dtype=np.complex128)\n",
    "    U=givens_matrix(n,p1[0],p1[1],th1,ph1)@U\n",
    "    U=givens_matrix(n,p2[0],p2[1],th2,ph2)@U\n",
    "    return U\n",
    "\n",
    "def canon_phase(U):\n",
    "    a=np.angle(np.linalg.det(U))/U.shape[0]\n",
    "    return U*np.exp(-1j*a)\n",
    "\n",
    "def vec_features(U):\n",
    "    return np.concatenate([U.real.reshape(-1),U.imag.reshape(-1)],0).astype(np.float32)\n",
    "\n",
    "def pack_angles(theta,phi):\n",
    "    return np.array([np.cos(theta),np.sin(theta),np.cos(phi),np.sin(phi)],dtype=np.float32)\n",
    "\n",
    "def unpack_angles(y):\n",
    "    cth,sth,cph,sph=y\n",
    "    theta=np.arctan2(sth,cth); phi=np.arctan2(sph,cph)\n",
    "    theta=np.mod(theta,math.pi/2); phi=np.mod(phi,2*math.pi)\n",
    "    return theta,phi\n",
    "\n",
    "def fidelity(U,Uhat):\n",
    "    n=U.shape[0]\n",
    "    return float(np.abs(np.trace(U.conj().T@Uhat))/n)\n",
    "\n",
    "class Seq2Dataset(Dataset):\n",
    "    def __init__(self,n,pairs,size,seed=0):\n",
    "        self.n=n; self.pairs=pairs; self.size=size\n",
    "        rng=np.random.default_rng(seed)\n",
    "        X=[]; y_p1=[]; y_p2=[]; y_a1=[]; y_a2=[]\n",
    "        for _ in range(size):\n",
    "            k1=rng.integers(0,len(pairs)); k2=rng.integers(0,len(pairs))\n",
    "            th1=rng.uniform(0,math.pi/2); ph1=rng.uniform(0,2*math.pi)\n",
    "            th2=rng.uniform(0,math.pi/2); ph2=rng.uniform(0,2*math.pi)\n",
    "            U=apply_two(n,pairs[k1],th1,ph1,pairs[k2],th2,ph2)\n",
    "            U=canon_phase(U)\n",
    "            X.append(vec_features(U))\n",
    "            y_p1.append(k1); y_p2.append(k2)\n",
    "            y_a1.append(pack_angles(th1,ph1)); y_a2.append(pack_angles(th2,ph2))\n",
    "        self.X=np.stack(X,0).astype(np.float32)\n",
    "        self.y_p1=np.array(y_p1,dtype=np.int64)\n",
    "        self.y_p2=np.array(y_p2,dtype=np.int64)\n",
    "        self.y_a1=np.stack(y_a1,0).astype(np.float32)\n",
    "        self.y_a2=np.stack(y_a2,0).astype(np.float32)\n",
    "    def __len__(self): return self.size\n",
    "    def __getitem__(self,idx):\n",
    "        return torch.from_numpy(self.X[idx]), torch.tensor(self.y_p1[idx]), torch.tensor(self.y_p2[idx]), torch.from_numpy(self.y_a1[idx]), torch.from_numpy(self.y_a2[idx])\n",
    "\n",
    "class Trunk(nn.Module):\n",
    "    def __init__(self,in_dim,h=512):\n",
    "        super().__init__()\n",
    "        self.net=nn.Sequential(nn.Linear(in_dim,h),nn.ReLU(),nn.Linear(h,h),nn.ReLU())\n",
    "    def forward(self,x): return self.net(x)\n",
    "\n",
    "class Seq2Net(nn.Module):\n",
    "    def __init__(self,in_dim,num_pairs,h=512):\n",
    "        super().__init__()\n",
    "        self.trunk=Trunk(in_dim,h)\n",
    "        self.head_p1=nn.Linear(h,num_pairs)\n",
    "        self.head_p2=nn.Linear(h,num_pairs)\n",
    "        self.head_ang=nn.Linear(h,2*num_pairs*4)\n",
    "        self.num_pairs=num_pairs\n",
    "    def forward(self,x):\n",
    "        z=self.trunk(x)\n",
    "        l1=self.head_p1(z)\n",
    "        l2=self.head_p2(z)\n",
    "        a=self.head_ang(z).view(-1,2,self.num_pairs,4)\n",
    "        return l1,l2,a\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_fid(model,loader,n,pairs,device):\n",
    "    model.eval()\n",
    "    N=0; sf=0.0; ltot=0.0\n",
    "    for xb,y_p1,y_p2,y_a1,y_a2 in loader:\n",
    "        xb=xb.to(device); y_p1=y_p1.to(device); y_p2=y_p2.to(device); y_a1=y_a1.to(device); y_a2=y_a2.to(device)\n",
    "        l1,l2,angs=model(xb)\n",
    "        ce1=F.cross_entropy(l1,y_p1); ce2=F.cross_entropy(l2,y_p2)\n",
    "        a1=angs[:,0]; a2=angs[:,1]\n",
    "        idx1=y_p1.view(-1,1,1).expand(-1,1,4)\n",
    "        idx2=y_p2.view(-1,1,1).expand(-1,1,4)\n",
    "        sel1=torch.gather(a1,1,idx1).squeeze(1)\n",
    "        sel2=torch.gather(a2,1,idx2).squeeze(1)\n",
    "        mse=F.mse_loss(sel1,y_a1)+F.mse_loss(sel2,y_a2)\n",
    "        loss=ce1+ce2+mse\n",
    "        p1=l1.argmax(dim=1); p2=l2.argmax(dim=1)\n",
    "        pa1=torch.gather(a1,1,p1.view(-1,1,1).expand(-1,1,4)).squeeze(1).cpu().numpy()\n",
    "        pa2=torch.gather(a2,1,p2.view(-1,1,1).expand(-1,1,4)).squeeze(1).cpu().numpy()\n",
    "        ta1=y_a1.cpu().numpy(); ta2=y_a2.cpu().numpy()\n",
    "        p1=p1.cpu().numpy(); p2=p2.cpu().numpy(); y_p1_np=y_p1.cpu().numpy(); y_p2_np=y_p2.cpu().numpy()\n",
    "        for b in range(xb.size(0)):\n",
    "            th1_t,ph1_t=unpack_angles(ta1[b]); th2_t,ph2_t=unpack_angles(ta2[b])\n",
    "            th1_p,ph1_p=unpack_angles(pa1[b]); th2_p,ph2_p=unpack_angles(pa2[b])\n",
    "            U_t=canon_phase(apply_two(n,pairs[y_p1_np[b]],th1_t,ph1_t,pairs[y_p2_np[b]],th2_t,ph2_t))\n",
    "            U_p=canon_phase(apply_two(n,pairs[p1[b]],th1_p,ph1_p,pairs[p2[b]],th2_p,ph2_p))\n",
    "            sf+=fidelity(U_t,U_p); N+=1\n",
    "        ltot+=loss.item()*xb.size(0)\n",
    "    return ltot/max(1,N), sf/max(1,N)\n",
    "\n",
    "def train_seq2_selector(n=4,pairs=[(0,1),(0,2),(0,3)],epochs=20,train_size=24000,val_size=3000,batch=256,lr=1e-3,seed=0,device='cpu'):\n",
    "    torch.manual_seed(seed); np.random.seed(seed)\n",
    "    train_ds=Seq2Dataset(n,pairs,train_size,seed)\n",
    "    val_ds=Seq2Dataset(n,pairs,val_size,seed+1)\n",
    "    in_dim=2*n*n; P=len(pairs)\n",
    "    model=Seq2Net(in_dim,P,512).to(device)\n",
    "    opt=torch.optim.AdamW(model.parameters(),lr=lr)\n",
    "    train_loader=DataLoader(train_ds,batch_size=batch,shuffle=True,drop_last=True)\n",
    "    val_loader=DataLoader(val_ds,batch_size=batch,shuffle=False)\n",
    "    best_fid=-1.0; best_state=None\n",
    "    for ep in range(1,epochs+1):\n",
    "        model.train()\n",
    "        tot=0.0; cnt=0\n",
    "        for xb,y_p1,y_p2,y_a1,y_a2 in train_loader:\n",
    "            xb=xb.to(device); y_p1=y_p1.to(device); y_p2=y_p2.to(device); y_a1=y_a1.to(device); y_a2=y_a2.to(device)\n",
    "            l1,l2,angs=model(xb)\n",
    "            ce1=F.cross_entropy(l1,y_p1); ce2=F.cross_entropy(l2,y_p2)\n",
    "            a1=angs[:,0]; a2=angs[:,1]\n",
    "            idx1=y_p1.view(-1,1,1).expand(-1,1,4)\n",
    "            idx2=y_p2.view(-1,1,1).expand(-1,1,4)\n",
    "            sel1=torch.gather(a1,1,idx1).squeeze(1)\n",
    "            sel2=torch.gather(a2,1,idx2).squeeze(1)\n",
    "            mse=F.mse_loss(sel1,y_a1)+F.mse_loss(sel2,y_a2)\n",
    "            loss=ce1+ce2+mse\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "            tot+=loss.item()*xb.size(0); cnt+=xb.size(0)\n",
    "        train_loss=tot/cnt\n",
    "        vloss,vfid=eval_fid(model,val_loader,n,pairs,device)\n",
    "        tloss,tfid=eval_fid(model,DataLoader(train_ds,batch_size=batch,shuffle=False),n,pairs,device)\n",
    "        if vfid>best_fid:\n",
    "            best_fid=vfid\n",
    "            best_state={k:v.detach().cpu() for k,v in model.state_dict().items()}\n",
    "        print(f\"epoch {ep:02d} | train_loss {train_loss:.5f} val_loss {vloss:.5f} | train_fid {tfid:.6f} val_fid {vfid:.6f}\")\n",
    "    model.load_state_dict(best_state)\n",
    "    meta={'n':n,'pairs':pairs}\n",
    "    torch.save({'state_dict':model.state_dict(),'meta':meta},'seq2_selector.pt')\n",
    "    return model,meta\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_seq2(model,meta,U):\n",
    "    n=meta['n']; pairs=meta['pairs']\n",
    "    x=torch.from_numpy(vec_features(canon_phase(U))).unsqueeze(0).float()\n",
    "    l1,l2,angs=model(x)\n",
    "    k1=int(torch.argmax(l1,dim=1).item())\n",
    "    k2=int(torch.argmax(l2,dim=1).item())\n",
    "    y1=angs[0,0,k1].numpy(); y2=angs[0,1,k2].numpy()\n",
    "    th1,ph1=unpack_angles(y1); th2,ph2=unpack_angles(y2)\n",
    "    Uhat=canon_phase(apply_two(n,pairs[k1],th1,ph1,pairs[k2],th2,ph2))\n",
    "    return [{'pair':pairs[k1],'theta':float(th1),'phi':float(ph1)},{'pair':pairs[k2],'theta':float(th2),'phi':float(ph2)}],Uhat\n",
    "\n",
    "def demo_seq2(n=4,pairs=[(0,1),(0,2),(0,3)],seed=123):\n",
    "    rng=np.random.default_rng(seed)\n",
    "    k1=int(rng.integers(0,len(pairs))); k2=int(rng.integers(0,len(pairs)))\n",
    "    th1=rng.uniform(0,math.pi/2); ph1=rng.uniform(0,2*math.pi)\n",
    "    th2=rng.uniform(0,math.pi/2); ph2=rng.uniform(0,2*math.pi)\n",
    "    U=canon_phase(apply_two(n,pairs[k1],th1,ph1,pairs[k2],th2,ph2))\n",
    "    ckpt=torch.load('seq2_selector.pt',map_location='cpu')\n",
    "    in_dim=2*n*n; P=len(pairs)\n",
    "    model=Seq2Net(in_dim,P,512); model.load_state_dict(ckpt['state_dict']); model.eval()\n",
    "    seq_pred,U_pred=predict_seq2(model,{'n':n,'pairs':pairs},U)\n",
    "    fid_val=fidelity(U,U_pred)\n",
    "    return {'true_seq':[{'pair':pairs[k1],'theta':float(th1),'phi':float(ph1)},{'pair':pairs[k2],'theta':float(th2),'phi':float(ph2)}],'pred_seq':seq_pred,'fidelity':fid_val,'U_true':U,'U_pred':U_pred}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fe1df562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01 | train_loss 0.91891 val_loss 0.42568 | train_fid 0.967728 val_fid 0.967711\n",
      "epoch 02 | train_loss 0.35398 val_loss 0.31198 | train_fid 0.975277 val_fid 0.975238\n",
      "epoch 03 | train_loss 0.28857 val_loss 0.27858 | train_fid 0.969514 val_fid 0.969717\n",
      "epoch 04 | train_loss 0.25473 val_loss 0.23308 | train_fid 0.976999 val_fid 0.976556\n",
      "epoch 05 | train_loss 0.23588 val_loss 0.24008 | train_fid 0.970834 val_fid 0.973420\n",
      "epoch 06 | train_loss 0.22239 val_loss 0.21252 | train_fid 0.970548 val_fid 0.969789\n",
      "epoch 07 | train_loss 0.21217 val_loss 0.20606 | train_fid 0.971939 val_fid 0.970957\n",
      "epoch 08 | train_loss 0.20186 val_loss 0.20063 | train_fid 0.981481 val_fid 0.982207\n",
      "epoch 09 | train_loss 0.19498 val_loss 0.19468 | train_fid 0.980833 val_fid 0.980720\n",
      "epoch 10 | train_loss 0.19106 val_loss 0.19006 | train_fid 0.976722 val_fid 0.975669\n",
      "{\n",
      "  \"true_seq\": [\n",
      "    {\n",
      "      \"pair\": [\n",
      "        0,\n",
      "        3\n",
      "      ],\n",
      "      \"theta\": 1.4093401429126966,\n",
      "      \"phi\": 4.873776931938056\n",
      "    },\n",
      "    {\n",
      "      \"pair\": [\n",
      "        0,\n",
      "        2\n",
      "      ],\n",
      "      \"theta\": 0.35375462680502207,\n",
      "      \"phi\": 1.8860003910648933\n",
      "    }\n",
      "  ],\n",
      "  \"pred_seq\": [\n",
      "    {\n",
      "      \"pair\": [\n",
      "        0,\n",
      "        3\n",
      "      ],\n",
      "      \"theta\": 1.4377645254135132,\n",
      "      \"phi\": 4.738161087036133\n",
      "    },\n",
      "    {\n",
      "      \"pair\": [\n",
      "        0,\n",
      "        2\n",
      "      ],\n",
      "      \"theta\": 0.44246408343315125,\n",
      "      \"phi\": 1.9180634021759033\n",
      "    }\n",
      "  ],\n",
      "  \"fidelity\": 0.9931549187245752\n",
      "}\n",
      "U_true:\n",
      " [[ 0.1508+0.j      0.    +0.j      0.1074-0.3294j -0.1488+0.9138j]\n",
      " [ 0.    +0.j      1.    +0.j      0.    +0.j      0.    +0.j    ]\n",
      " [-0.0173-0.0529j  0.    +0.j      0.9381+0.j      0.3379-0.0524j]\n",
      " [ 0.1586+0.9742j  0.    +0.j      0.    +0.j      0.1608+0.j    ]]\n",
      "U_pred:\n",
      " [[ 0.1199+0.j      0.    +0.j      0.1457-0.4026j -0.0231+0.8954j]\n",
      " [ 0.    +0.j      1.    +0.j      0.    +0.j      0.    +0.j    ]\n",
      " [-0.0193-0.0534j  0.    +0.j      0.9037+0.j      0.4026-0.1341j]\n",
      " [ 0.0255+0.9908j  0.    +0.j      0.    +0.j      0.1326+0.j    ]]\n"
     ]
    }
   ],
   "source": [
    "model, meta = train_seq2_selector(n=4, pairs=[(0,1),(0,2),(0,3)], epochs=10, train_size=60000, val_size=6000, batch=256, lr=3*1e-3, seed=0, device='cuda')\n",
    "res = demo_seq2(n=4, pairs=[(0,1),(0,2),(0,3)], seed=7)\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "print(json.dumps({k:res[k] for k in ['true_seq','pred_seq','fidelity']}, indent=2))\n",
    "print('U_true:\\n', res['U_true'])\n",
    "print('U_pred:\\n', res['U_pred'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "33e3426c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"true_seq\": [\n",
      "    {\n",
      "      \"pair\": [\n",
      "        0,\n",
      "        2\n",
      "      ],\n",
      "      \"theta\": 1.492984882940679,\n",
      "      \"phi\": 0.9057815605287021\n",
      "    },\n",
      "    {\n",
      "      \"pair\": [\n",
      "        0,\n",
      "        2\n",
      "      ],\n",
      "      \"theta\": 1.490135066979192,\n",
      "      \"phi\": 1.9592947975887585\n",
      "    }\n",
      "  ],\n",
      "  \"pred_seq\": [\n",
      "    {\n",
      "      \"pair\": [\n",
      "        0,\n",
      "        2\n",
      "      ],\n",
      "      \"theta\": 1.3921685218811035,\n",
      "      \"phi\": 2.736405611038208\n",
      "    },\n",
      "    {\n",
      "      \"pair\": [\n",
      "        0,\n",
      "        2\n",
      "      ],\n",
      "      \"theta\": 1.4535804986953735,\n",
      "      \"phi\": 1.4361897706985474\n",
      "    }\n",
      "  ],\n",
      "  \"fidelity\": 0.16564087345883183\n",
      "}\n",
      "U_true:\n",
      " [[-0.4852-0.8637j  0.    +0.j     -0.0202-0.1349j  0.    +0.j    ]\n",
      " [ 0.    +0.j      1.    +0.j      0.    +0.j      0.    +0.j    ]\n",
      " [ 0.0202-0.1349j  0.    +0.j     -0.4852+0.8637j  0.    +0.j    ]\n",
      " [ 0.    +0.j      0.    +0.j      0.    +0.j      1.    +0.j    ]]\n",
      "U_pred:\n",
      " [[-0.2405+0.9418j  0.    +0.j      0.0821-0.2202j  0.    +0.j    ]\n",
      " [ 0.    +0.j      1.    +0.j      0.    +0.j      0.    +0.j    ]\n",
      " [-0.0821-0.2202j  0.    +0.j     -0.2405-0.9418j  0.    +0.j    ]\n",
      " [ 0.    +0.j      0.    +0.j      0.    +0.j      1.    +0.j    ]]\n"
     ]
    }
   ],
   "source": [
    "res = demo_seq2(n=4, pairs=[(0,1),(0,2),(0,3)], seed=1)\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "print(json.dumps({k:res[k] for k in ['true_seq','pred_seq','fidelity']}, indent=2))\n",
    "print('U_true:\\n', res['U_true'])\n",
    "print('U_pred:\\n', res['U_pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636255a4",
   "metadata": {},
   "source": [
    "## One and Two rotations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5a29d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, json, numpy as np, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def givens_matrix(n,i,j,theta,phi):\n",
    "    G=np.eye(n,dtype=np.complex128)\n",
    "    c=np.cos(theta); s=np.sin(theta); e=np.exp(1j*phi)\n",
    "    G[i,i]=c; G[j,j]=c; G[i,j]=-e*s; G[j,i]=np.conj(e)*s\n",
    "    return G\n",
    "\n",
    "def apply_one(n,p1,th1,ph1):\n",
    "    U=np.eye(n,dtype=np.complex128)\n",
    "    U=givens_matrix(n,p1[0],p1[1],th1,ph1)@U\n",
    "    return U\n",
    "\n",
    "def apply_two(n,p1,th1,ph1,p2,th2,ph2):\n",
    "    U=np.eye(n,dtype=np.complex128)\n",
    "    U=givens_matrix(n,p1[0],p1[1],th1,ph1)@U\n",
    "    U=givens_matrix(n,p2[0],p2[1],th2,ph2)@U\n",
    "    return U\n",
    "\n",
    "def canon_phase(U):\n",
    "    a=np.angle(np.linalg.det(U))/U.shape[0]\n",
    "    return U*np.exp(-1j*a)\n",
    "\n",
    "def vec_features(U):\n",
    "    return np.concatenate([U.real.reshape(-1),U.imag.reshape(-1)],0).astype(np.float32)\n",
    "\n",
    "def pack_angles(theta,phi):\n",
    "    return np.array([np.cos(theta),np.sin(theta),np.cos(phi),np.sin(phi)],dtype=np.float32)\n",
    "\n",
    "def unpack_angles(y):\n",
    "    cth,sth,cph,sph=y\n",
    "    theta=np.arctan2(sth,cth); phi=np.arctan2(sph,cph)\n",
    "    theta=np.mod(theta,math.pi/2); phi=np.mod(phi,2*math.pi)\n",
    "    return theta,phi\n",
    "\n",
    "def fidelity(U,Uhat):\n",
    "    n=U.shape[0]\n",
    "    return float(np.abs(np.trace(U.conj().T@Uhat))/n)\n",
    "\n",
    "class SeqMixDataset(Dataset):\n",
    "    def __init__(self,n,pairs,size,p_one=0.5,seed=0):\n",
    "        self.n=n; self.pairs=pairs; self.size=size; self.p_one=p_one; self.STOP=len(pairs)\n",
    "        rng=np.random.default_rng(seed)\n",
    "        X=[]; y_p1=[]; y_p2=[]; y_a1=[]; y_a2=[]\n",
    "        for _ in range(size):\n",
    "            k1=rng.integers(0,len(pairs))\n",
    "            th1=rng.uniform(0,math.pi/2); ph1=rng.uniform(0,2*math.pi)\n",
    "            if rng.random()<p_one:\n",
    "                U=apply_one(n,pairs[k1],th1,ph1)\n",
    "                k2=self.STOP; th2=0.0; ph2=0.0\n",
    "            else:\n",
    "                k2=rng.integers(0,len(pairs))\n",
    "                th2=rng.uniform(0,math.pi/2); ph2=rng.uniform(0,2*math.pi)\n",
    "                U=apply_two(n,pairs[k1],th1,ph1,pairs[k2],th2,ph2)\n",
    "            U=canon_phase(U)\n",
    "            X.append(vec_features(U))\n",
    "            y_p1.append(k1); y_p2.append(k2)\n",
    "            y_a1.append(pack_angles(th1,ph1))\n",
    "            y_a2.append(pack_angles(th2,ph2) if k2!=self.STOP else np.array([1.,0.,1.,0.],dtype=np.float32))\n",
    "        self.X=np.stack(X,0).astype(np.float32)\n",
    "        self.y_p1=np.array(y_p1,dtype=np.int64)\n",
    "        self.y_p2=np.array(y_p2,dtype=np.int64)\n",
    "        self.y_a1=np.stack(y_a1,0).astype(np.float32)\n",
    "        self.y_a2=np.stack(y_a2,0).astype(np.float32)\n",
    "    def __len__(self): return self.size\n",
    "    def __getitem__(self,idx):\n",
    "        return (torch.from_numpy(self.X[idx]),\n",
    "                torch.tensor(self.y_p1[idx]),\n",
    "                torch.tensor(self.y_p2[idx]),\n",
    "                torch.from_numpy(self.y_a1[idx]),\n",
    "                torch.from_numpy(self.y_a2[idx]))\n",
    "\n",
    "class Trunk(nn.Module):\n",
    "    def __init__(self,in_dim,h=512):\n",
    "        super().__init__()\n",
    "        self.net=nn.Sequential(nn.Linear(in_dim,h),nn.ReLU(),nn.Linear(h,h),nn.ReLU())\n",
    "    def forward(self,x): return self.net(x)\n",
    "\n",
    "class Seq2StopNet(nn.Module):\n",
    "    def __init__(self,in_dim,num_pairs,h=512):\n",
    "        super().__init__()\n",
    "        self.trunk=Trunk(in_dim,h)\n",
    "        self.head_p1=nn.Linear(h,num_pairs)\n",
    "        self.head_p2=nn.Linear(h,num_pairs+1)\n",
    "        self.head_ang=nn.Linear(h,2*num_pairs*4)\n",
    "        self.num_pairs=num_pairs\n",
    "    def forward(self,x):\n",
    "        z=self.trunk(x)\n",
    "        l1=self.head_p1(z)\n",
    "        l2=self.head_p2(z)\n",
    "        a=self.head_ang(z).view(-1,2,self.num_pairs,4)\n",
    "        return l1,l2,a\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_stats(model,loader,n,pairs,STOP,device):\n",
    "    model.eval()\n",
    "    N=0; sf=0.0; ltot=0.0; used_sum=0.0\n",
    "    for xb,y_p1,y_p2,y_a1,y_a2 in loader:\n",
    "        xb=xb.to(device); y_p1=y_p1.to(device); y_p2=y_p2.to(device); y_a1=y_a1.to(device); y_a2=y_a2.to(device)\n",
    "        l1,l2,angs=model(xb)\n",
    "        ce1=F.cross_entropy(l1,y_p1); ce2=F.cross_entropy(l2,y_p2)\n",
    "        a1=angs[:,0]; a2=angs[:,1]\n",
    "        idx1=y_p1.view(-1,1,1).expand(-1,1,4)\n",
    "        sel1=torch.gather(a1,1,idx1).squeeze(1)\n",
    "        mask=(y_p2!=STOP).float().unsqueeze(-1)\n",
    "        idx2=torch.clamp(y_p2,0,STOP-1).view(-1,1,1).expand(-1,1,4)\n",
    "        sel2=torch.gather(a2,1,idx2).squeeze(1)\n",
    "        mse=F.mse_loss(sel1,y_a1)+F.mse_loss(sel2,y_a2,reduction='none')\n",
    "        mse=(mse*mask).mean()\n",
    "        loss=ce1+ce2+mse\n",
    "        probs=torch.softmax(l2,dim=1)\n",
    "        used=1.0-(probs[:,STOP]).mean().item()\n",
    "        p1=l1.argmax(dim=1); p2=l2.argmax(dim=1)\n",
    "        pa1=torch.gather(a1,1,p1.view(-1,1,1).expand(-1,1,4)).squeeze(1).cpu().numpy()\n",
    "        pa2=torch.gather(a2,1,torch.clamp(p2,0,STOP-1).view(-1,1,1).expand(-1,1,4)).squeeze(1).cpu().numpy()\n",
    "        ta1=y_a1.cpu().numpy(); ta2=y_a2.cpu().numpy()\n",
    "        p1=p1.cpu().numpy(); p2=p2.cpu().numpy(); y_p1_np=y_p1.cpu().numpy(); y_p2_np=y_p2.cpu().numpy()\n",
    "        for b in range(xb.size(0)):\n",
    "            th1_t,ph1_t=unpack_angles(ta1[b])\n",
    "            U_t=apply_one(n,pairs[y_p1_np[b]],th1_t,ph1_t)\n",
    "            if y_p2_np[b]!=STOP:\n",
    "                th2_t,ph2_t=unpack_angles(ta2[b])\n",
    "                U_t=apply_two(n,pairs[y_p1_np[b]],th1_t,ph1_t,pairs[y_p2_np[b]],th2_t,ph2_t)\n",
    "            U_t=canon_phase(U_t)\n",
    "            th1_p,ph1_p=unpack_angles(pa1[b])\n",
    "            U_p=apply_one(n,pairs[p1[b]],th1_p,ph1_p)\n",
    "            if p2[b]!=STOP:\n",
    "                th2_p,ph2_p=unpack_angles(pa2[b])\n",
    "                U_p=apply_two(n,pairs[p1[b]],th1_p,ph1_p,pairs[p2[b]],th2_p,ph2_p)\n",
    "            U_p=canon_phase(U_p)\n",
    "            sf+=fidelity(U_t,U_p); N+=1\n",
    "        ltot+=loss.item()*xb.size(0); used_sum+=used*xb.size(0)\n",
    "    return ltot/max(1,N), sf/max(1,N), used_sum/max(1,N)\n",
    "\n",
    "def train_seq2_minrot(n=4,pairs=[(0,1),(0,2),(0,3)],epochs=20,train_size=24000,val_size=3000,batch=256,lr=1e-3,p_one=0.5,lam_use=0.01,seed=0,device='cpu'):\n",
    "    torch.manual_seed(seed); np.random.seed(seed)\n",
    "    STOP=len(pairs)\n",
    "    train_ds=SeqMixDataset(n,pairs,train_size,p_one=p_one,seed=seed)\n",
    "    val_ds=SeqMixDataset(n,pairs,val_size,p_one=p_one,seed=seed+1)\n",
    "    in_dim=2*n*n; P=len(pairs)\n",
    "    model=Seq2StopNet(in_dim,P,512).to(device)\n",
    "    opt=torch.optim.AdamW(model.parameters(),lr=lr)\n",
    "    train_loader=DataLoader(train_ds,batch_size=batch,shuffle=True,drop_last=True)\n",
    "    val_loader=DataLoader(val_ds,batch_size=batch,shuffle=False)\n",
    "    best_fid=-1.0; best_state=None\n",
    "    for ep in range(1,epochs+1):\n",
    "        model.train()\n",
    "        tot=0.0; cnt=0\n",
    "        for xb,y_p1,y_p2,y_a1,y_a2 in train_loader:\n",
    "            xb=xb.to(device); y_p1=y_p1.to(device); y_p2=y_p2.to(device); y_a1=y_a1.to(device); y_a2=y_a2.to(device)\n",
    "            l1,l2,angs=model(xb)\n",
    "            ce1=F.cross_entropy(l1,y_p1); ce2=F.cross_entropy(l2,y_p2)\n",
    "            a1=angs[:,0]; a2=angs[:,1]\n",
    "            idx1=y_p1.view(-1,1,1).expand(-1,1,4)\n",
    "            sel1=torch.gather(a1,1,idx1).squeeze(1)\n",
    "            mask=(y_p2!=STOP).float().unsqueeze(-1)\n",
    "            idx2=torch.clamp(y_p2,0,STOP-1).view(-1,1,1).expand(-1,1,4)\n",
    "            sel2=torch.gather(a2,1,idx2).squeeze(1)\n",
    "            mse=F.mse_loss(sel1,y_a1)+F.mse_loss(sel2,y_a2,reduction='none')\n",
    "            mse=(mse*mask).mean()\n",
    "            probs=torch.softmax(l2,dim=1)\n",
    "            use_pen=1.0-(probs[:,STOP]).mean()\n",
    "            loss=ce1+ce2+mse+lam_use*use_pen\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "            tot+=loss.item()*xb.size(0); cnt+=xb.size(0)\n",
    "        train_loss=tot/cnt\n",
    "        vloss,vfid,vused=eval_stats(model,val_loader,n,pairs,STOP,device)\n",
    "        tloss,tfid,tused=eval_stats(model,DataLoader(train_ds,batch_size=batch,shuffle=False),n,pairs,STOP,device)\n",
    "        if vfid>best_fid:\n",
    "            best_fid=vfid\n",
    "            best_state={k:v.detach().cpu() for k,v in model.state_dict().items()}\n",
    "        print(f\"epoch {ep:02d} | train_loss {train_loss:.5f} val_loss {vloss:.5f} | train_fid {tfid:.6f} val_fid {vfid:.6f} | train_used {tused:.3f} val_used {vused:.3f}\")\n",
    "    model.load_state_dict(best_state)\n",
    "    meta={'n':n,'pairs':pairs,'STOP':STOP}\n",
    "    torch.save({'state_dict':model.state_dict(),'meta':meta},'seq2_minrot.pt')\n",
    "    return model,meta\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_seq2_minrot(model,meta,U):\n",
    "    n=meta['n']; pairs=meta['pairs']; STOP=meta['STOP']\n",
    "    x=torch.from_numpy(vec_features(canon_phase(U))).unsqueeze(0).float()\n",
    "    l1,l2,angs=model(x)\n",
    "    k1=int(torch.argmax(l1,dim=1).item())\n",
    "    k2=int(torch.argmax(l2,dim=1).item())\n",
    "    y1=angs[0,0,k1].numpy()\n",
    "    th1,ph1=unpack_angles(y1)\n",
    "    seq=[{'pair':pairs[k1],'theta':float(th1),'phi':float(ph1)}]\n",
    "    Uhat=apply_one(n,pairs[k1],th1,ph1)\n",
    "    if k2!=STOP:\n",
    "        y2=angs[0,1,k2 if k2<STOP else STOP-1].numpy()\n",
    "        th2,ph2=unpack_angles(y2)\n",
    "        seq.append({'pair':pairs[k2],'theta':float(th2),'phi':float(ph2)})\n",
    "        Uhat=apply_two(n,pairs[k1],th1,ph1,pairs[k2],th2,ph2)\n",
    "    Uhat=canon_phase(Uhat)\n",
    "    return seq,Uhat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8790fc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01 | train_loss 1.27057 val_loss 0.74370 | train_fid 0.972146 val_fid 0.973039 | train_used 0.341 val_used 0.343\n",
      "epoch 02 | train_loss 0.52166 val_loss 0.40670 | train_fid 0.982740 val_fid 0.983426 | train_used 0.433 val_used 0.435\n",
      "epoch 03 | train_loss 0.34702 val_loss 0.29866 | train_fid 0.990710 val_fid 0.990824 | train_used 0.385 val_used 0.388\n",
      "epoch 04 | train_loss 0.28176 val_loss 0.25096 | train_fid 0.990808 val_fid 0.991037 | train_used 0.395 val_used 0.398\n",
      "epoch 05 | train_loss 0.24745 val_loss 0.24546 | train_fid 0.988825 val_fid 0.989606 | train_used 0.397 val_used 0.399\n",
      "epoch 06 | train_loss 0.22000 val_loss 0.21045 | train_fid 0.986473 val_fid 0.986839 | train_used 0.394 val_used 0.396\n",
      "epoch 07 | train_loss 0.20324 val_loss 0.19071 | train_fid 0.990806 val_fid 0.991205 | train_used 0.400 val_used 0.402\n",
      "epoch 08 | train_loss 0.19467 val_loss 0.19008 | train_fid 0.988446 val_fid 0.989261 | train_used 0.398 val_used 0.401\n",
      "epoch 09 | train_loss 0.17906 val_loss 0.17791 | train_fid 0.989558 val_fid 0.989874 | train_used 0.411 val_used 0.413\n",
      "epoch 10 | train_loss 0.17493 val_loss 0.17348 | train_fid 0.989130 val_fid 0.990263 | train_used 0.396 val_used 0.397\n",
      "epoch 11 | train_loss 0.16698 val_loss 0.17479 | train_fid 0.987344 val_fid 0.988274 | train_used 0.409 val_used 0.412\n",
      "epoch 12 | train_loss 0.15897 val_loss 0.15876 | train_fid 0.986723 val_fid 0.987663 | train_used 0.412 val_used 0.415\n",
      "epoch 13 | train_loss 0.15372 val_loss 0.16219 | train_fid 0.990749 val_fid 0.990797 | train_used 0.389 val_used 0.392\n",
      "epoch 14 | train_loss 0.14739 val_loss 0.14203 | train_fid 0.984660 val_fid 0.986371 | train_used 0.396 val_used 0.398\n",
      "epoch 15 | train_loss 0.14087 val_loss 0.14914 | train_fid 0.977741 val_fid 0.977546 | train_used 0.412 val_used 0.414\n",
      "{\n",
      "  \"pred_seq\": [\n",
      "    {\n",
      "      \"pair\": [\n",
      "        0,\n",
      "        3\n",
      "      ],\n",
      "      \"theta\": 1.4410426616668701,\n",
      "      \"phi\": 4.878142356872559\n",
      "    }\n",
      "  ],\n",
      "  \"fid\": 0.9997440872500745\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "model, meta = train_seq2_minrot(n=4, pairs=[(0,1),(0,2),(0,3)], epochs=15, train_size=60000, val_size=10000, batch=256, lr=1e-3, p_one=0.6, lam_use=0.02, seed=0, device='cpu')\n",
    "\n",
    "rng=np.random.default_rng(7)\n",
    "pairs=[(0,1),(0,2),(0,3)]\n",
    "k1=int(rng.integers(0,len(pairs)))\n",
    "th1=rng.uniform(0,math.pi/2); ph1=rng.uniform(0,2*math.pi)\n",
    "if rng.random()<0.5:\n",
    "    U=canon_phase(apply_one(4,pairs[k1],th1,ph1))\n",
    "else:\n",
    "    k2=int(rng.integers(0,len(pairs)))\n",
    "    th2=rng.uniform(0,math.pi/2); ph2=rng.uniform(0,2*math.pi)\n",
    "    U=canon_phase(apply_two(4,pairs[k1],th1,ph1,pairs[k2],th2,ph2))\n",
    "\n",
    "seq_pred,U_pred=predict_seq2_minrot(model,meta,U)\n",
    "fid=fidelity(U,U_pred)\n",
    "print(json.dumps({'pred_seq':seq_pred,'fid':fid}, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "52975706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True U: \n",
      " [[ 0.+0.j  0.+0.j  0.+0.j -1.+0.j]\n",
      " [ 0.+0.j  1.+0.j  0.+0.j  0.+0.j]\n",
      " [ 0.+0.j  0.+0.j  1.+0.j  0.+0.j]\n",
      " [ 1.+0.j  0.+0.j  0.+0.j  0.+0.j]]\n",
      "Pred U: \n",
      " [[ 0.0059+0.j      0.    +0.j      0.    +0.j     -0.9966+0.0824j]\n",
      " [ 0.    +0.j      1.    +0.j      0.    +0.j      0.    +0.j    ]\n",
      " [ 0.    +0.j      0.    +0.j      1.    +0.j      0.    +0.j    ]\n",
      " [ 0.9966+0.0824j  0.    +0.j      0.    +0.j      0.0059+0.j    ]]\n",
      "{\n",
      "  \"pred_seq\": [\n",
      "    {\n",
      "      \"pair\": [\n",
      "        0,\n",
      "        3\n",
      "      ],\n",
      "      \"theta\": 1.564945936203003,\n",
      "      \"phi\": 6.200651168823242\n",
      "    }\n",
      "  ],\n",
      "  \"fid\": 0.998289465904236\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "rng=np.random.default_rng(10)\n",
    "pairs=[(0,1),(0,2),(0,3)]\n",
    "k1=int(rng.integers(0,len(pairs)))\n",
    "th1=rng.uniform(0,math.pi/2); ph1=rng.uniform(0,2*math.pi)\n",
    "if rng.random()<0.5:\n",
    "    U=canon_phase(apply_one(4,pairs[k1],th1,ph1))\n",
    "else:\n",
    "    k2=int(rng.integers(0,len(pairs)))\n",
    "    th2=rng.uniform(0,math.pi/2); ph2=rng.uniform(0,2*math.pi)\n",
    "    U=canon_phase(apply_two(4,pairs[k1],th1,ph1,pairs[k2],th2,ph2))\n",
    "\n",
    "k1 = 2\n",
    "k2 = 2\n",
    "th1 = math.pi/4\n",
    "th2 = math.pi/4\n",
    "ph1 = 0\n",
    "ph2 = 0\n",
    "\n",
    "U=canon_phase(apply_two(4,pairs[k1],th1,ph1,pairs[k2],th2,ph2))\n",
    "seq_pred,U_pred=predict_seq2_minrot(model,meta,U)\n",
    "print(\"True U: \\n\", U)\n",
    "print(\"Pred U: \\n\", U_pred)\n",
    "fid=fidelity(U,U_pred)\n",
    "print(json.dumps({'pred_seq':seq_pred,'fid':fid}, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d506fe3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b37fb8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, json, numpy as np, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def givens_matrix(n,i,j,theta,phi):\n",
    "    G=np.eye(n,dtype=np.complex128)\n",
    "    c=np.cos(theta); s=np.sin(theta); e=np.exp(1j*phi)\n",
    "    G[i,i]=c; G[j,j]=c; G[i,j]=-e*s; G[j,i]=np.conj(e)*s\n",
    "    return G\n",
    "\n",
    "def apply_seq(n,pairs,seq_pairs,seq_theta,seq_phi):\n",
    "    U=np.eye(n,dtype=np.complex128)\n",
    "    for k in range(len(seq_pairs)):\n",
    "        i,j=seq_pairs[k]\n",
    "        U=givens_matrix(n,i,j,seq_theta[k],seq_phi[k])@U\n",
    "    return U\n",
    "\n",
    "def canon_phase(U):\n",
    "    a=np.angle(np.linalg.det(U))/U.shape[0]\n",
    "    return U*np.exp(-1j*a)\n",
    "\n",
    "def vec_features(U):\n",
    "    return np.concatenate([U.real.reshape(-1),U.imag.reshape(-1)],0).astype(np.float32)\n",
    "\n",
    "def pack_angles(theta,phi):\n",
    "    return np.array([np.cos(theta),np.sin(theta),np.cos(phi),np.sin(phi)],dtype=np.float32)\n",
    "\n",
    "def unpack_angles(y):\n",
    "    cth,sth,cph,sph=y\n",
    "    theta=np.arctan2(sth,cth); phi=np.arctan2(sph,cph)\n",
    "    theta=np.mod(theta,math.pi/2); phi=np.mod(phi,2*math.pi)\n",
    "    return theta,phi\n",
    "\n",
    "def fidelity(U,Uhat):\n",
    "    n=U.shape[0]\n",
    "    return float(np.abs(np.trace(U.conj().T@Uhat))/n)\n",
    "\n",
    "def make_pairs(n,topology='star'):\n",
    "    if topology=='star':\n",
    "        return [(0,j) for j in range(1,n)]\n",
    "    if topology=='all':\n",
    "        return [(i,j) for i in range(n) for j in range(i+1,n)]\n",
    "    raise ValueError('unknown topology')\n",
    "\n",
    "class SeqTDataset(Dataset):\n",
    "    def __init__(self,n,depth,pairs,size,p_continue=0.4,seed=0):\n",
    "        self.n=n; self.depth=depth; self.pairs=pairs; self.size=size; self.P=len(pairs); self.STOP=self.P\n",
    "        rng=np.random.default_rng(seed)\n",
    "        X=[]; Yp=[]; Ya=[]\n",
    "        for _ in range(size):\n",
    "            L=1\n",
    "            for t in range(depth-1):\n",
    "                if rng.random()<p_continue: L+=1\n",
    "                else: break\n",
    "            L=min(L,depth)\n",
    "            ks=[]; ths=[]; phs=[]\n",
    "            for t in range(L):\n",
    "                k=int(rng.integers(0,self.P))\n",
    "                th=rng.uniform(0,math.pi); ph=rng.uniform(0,2*math.pi)\n",
    "                ks.append(k); ths.append(th); phs.append(ph)\n",
    "            U=apply_seq(n,pairs,[pairs[k] for k in ks],np.array(ths),np.array(phs))\n",
    "            U=canon_phase(U)\n",
    "            X.append(vec_features(U))\n",
    "            y_pairs=[ks[t] if t<L else self.STOP for t in range(depth)]\n",
    "            y_angles=[pack_angles(ths[t],phs[t]) if t<L else np.array([1.,0.,1.,0.],dtype=np.float32) for t in range(depth)]\n",
    "            Yp.append(y_pairs); Ya.append(y_angles)\n",
    "        self.X=np.stack(X,0).astype(np.float32)\n",
    "        self.Yp=np.array(Yp,dtype=np.int64)\n",
    "        self.Ya=np.array(Ya,dtype=np.float32)\n",
    "    def __len__(self): return self.size\n",
    "    def __getitem__(self,idx):\n",
    "        return torch.from_numpy(self.X[idx]), torch.from_numpy(self.Yp[idx]), torch.from_numpy(self.Ya[idx])\n",
    "\n",
    "class Trunk(nn.Module):\n",
    "    def __init__(self,in_dim,h=1024):\n",
    "        super().__init__()\n",
    "        self.net=nn.Sequential(nn.Linear(in_dim,h),nn.ReLU(),nn.Linear(h,h),nn.ReLU())\n",
    "    def forward(self,x): return self.net(x)\n",
    "\n",
    "class SeqTNet(nn.Module):\n",
    "    def __init__(self,in_dim,num_pairs,depth,h=1024):\n",
    "        super().__init__()\n",
    "        self.trunk=Trunk(in_dim,h)\n",
    "        self.head_pairs=nn.ModuleList([nn.Linear(h,num_pairs+1) for _ in range(depth)])\n",
    "        self.head_angles=nn.ModuleList([nn.Linear(h,num_pairs*4) for _ in range(depth)])\n",
    "        self.depth=depth; self.P=num_pairs\n",
    "    def forward(self,x):\n",
    "        z=self.trunk(x)\n",
    "        logits=[]; angs=[]\n",
    "        for t in range(self.depth):\n",
    "            lt=self.head_pairs[t](z)\n",
    "            at=self.head_angles[t](z).view(-1,self.P,4)\n",
    "            logits.append(lt); angs.append(at)\n",
    "        L=torch.stack(logits,dim=1)\n",
    "        A=torch.stack(angs,dim=1)\n",
    "        return L,A\n",
    "import torch, numpy as np, math\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def _norm2(x,eps=1e-8):\n",
    "    return x/torch.clamp(torch.linalg.norm(x,dim=-1,keepdim=True),min=eps)\n",
    "\n",
    "def _canon_phase_batch(U):\n",
    "    n=U.size(-1)\n",
    "    d=torch.linalg.det(U)\n",
    "    a=torch.angle(d)/n\n",
    "    f=torch.exp(-1j*a).unsqueeze(-1).unsqueeze(-1)\n",
    "    return U*f\n",
    "\n",
    "def _build_G(n,i_idx,j_idx,cth,sth,cph,sph,active):\n",
    "    device=cth.device\n",
    "    B,T=cth.shape\n",
    "    M=B*T\n",
    "    i=i_idx.reshape(M); j=j_idx.reshape(M)\n",
    "    c=cth.reshape(M); s=sth.reshape(M)\n",
    "    e=(cph.reshape(M)+1j*sph.reshape(M)).to(torch.complex64)\n",
    "    act=active.reshape(M)\n",
    "    G=torch.eye(n,device=device,dtype=torch.complex64).unsqueeze(0).expand(M,n,n).clone()\n",
    "    idx=torch.nonzero(act,as_tuple=False).squeeze(1)\n",
    "    if idx.numel()>0:\n",
    "        G[idx, i[idx], i[idx]] = c[idx].to(torch.complex64)\n",
    "        G[idx, j[idx], j[idx]] = c[idx].to(torch.complex64)\n",
    "        G[idx, i[idx], j[idx]] = -(e[idx]*s[idx])\n",
    "        G[idx, j[idx], i[idx]] = torch.conj(e[idx])*s[idx]\n",
    "    return G.view(B,T,n,n)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_stats(model,loader,n,pairs,STOP,device,max_batches=None):\n",
    "    model.eval()\n",
    "    P=len(pairs)\n",
    "    pi=torch.tensor([p[0] for p in pairs],device=device, dtype=torch.long)\n",
    "    pj=torch.tensor([p[1] for p in pairs],device=device, dtype=torch.long)\n",
    "    N=0; s_fid=0.0; s_used=0.0; s_loss=0.0; seen=0\n",
    "    for xb,y_pairs,y_angles in loader:\n",
    "        xb=xb.to(device, non_blocking=True).float()\n",
    "        y_pairs=y_pairs.to(device, non_blocking=True).long()\n",
    "        y_angles=y_angles.to(device, non_blocking=True).float()\n",
    "        L,A=model(xb)\n",
    "        depth=L.size(1)\n",
    "        ce=0.0\n",
    "        for t in range(depth):\n",
    "            ce+=torch.nn.functional.cross_entropy(L[:,t,:],y_pairs[:,t])\n",
    "        idx=torch.clamp(y_pairs,0,STOP-1).unsqueeze(-1).unsqueeze(-1).expand(-1,-1,1,4)\n",
    "        sel=torch.gather(A,2,idx).squeeze(2)\n",
    "        mask=(y_pairs!=STOP).float().unsqueeze(-1)\n",
    "        mse=torch.nn.functional.mse_loss(sel,y_angles,reduction='none')\n",
    "        mse=(mse*mask).mean()\n",
    "        loss=ce/depth+mse\n",
    "        s_loss+=loss.item()*xb.size(0)\n",
    "        probs=torch.softmax(L,dim=-1)\n",
    "        used=(1.0-probs[:,:,STOP]).mean().item()\n",
    "        s_used+=used*xb.size(0)\n",
    "        k_pred=L.argmax(dim=-1)\n",
    "        k_true=y_pairs\n",
    "        cp_sp=_norm2(sel[...,:2]); cph_sph=_norm2(sel[...,2:])\n",
    "        cth_t=cp_sp[...,0]; sth_t=cp_sp[...,1]\n",
    "        cph_t=cph_sph[...,0]; sph_t=cph_sph[...,1]\n",
    "        active_t=(k_true!=STOP)\n",
    "        ktc=torch.clamp(k_true,0,STOP-1)\n",
    "        it=pi[ktc]; jt=pj[ktc]\n",
    "        Gt=_build_G(n,it,jt,cth_t,sth_t,cph_t,sph_t,active_t)\n",
    "        B=xb.size(0)\n",
    "        U_t=torch.eye(n,device=device,dtype=torch.complex64).unsqueeze(0).expand(B,n,n).clone()\n",
    "        for t in range(depth):\n",
    "            U_t=torch.bmm(Gt[:,t],U_t)\n",
    "        U_t=_canon_phase_batch(U_t)\n",
    "        idxp=torch.clamp(k_pred,0,STOP-1).unsqueeze(-1).unsqueeze(-1).expand(-1,-1,1,4)\n",
    "        selp=torch.gather(A,2,idxp).squeeze(2)\n",
    "        cp_sp=_norm2(selp[...,:2]); cph_sph=_norm2(selp[...,2:])\n",
    "        cth_p=cp_sp[...,0]; sth_p=cp_sp[...,1]\n",
    "        cph_p=cph_sph[...,0]; sph_p=cph_sph[...,1]\n",
    "        active_p=(k_pred!=STOP)\n",
    "        itp=pi[torch.clamp(k_pred,0,STOP-1)]; jtp=pj[torch.clamp(k_pred,0,STOP-1)]\n",
    "        Gp=_build_G(n,itp,jtp,cth_p,sth_p,cph_p,sph_p,active_p)\n",
    "        U_p=torch.eye(n,device=device,dtype=torch.complex64).unsqueeze(0).expand(B,n,n).clone()\n",
    "        for t in range(depth):\n",
    "            U_p=torch.bmm(Gp[:,t],U_p)\n",
    "        U_p=_canon_phase_batch(U_p)\n",
    "        M=torch.matmul(torch.conj(U_t.transpose(-2,-1)),U_p)\n",
    "        tr=torch.diagonal(M,dim1=-2,dim2=-1).sum(-1)\n",
    "        fid=(tr.abs()/n).mean().item()\n",
    "        s_fid+=fid*xb.size(0)\n",
    "        N+=xb.size(0); seen+=1\n",
    "        if max_batches is not None and seen>=max_batches: break\n",
    "    return s_loss/max(1,N), s_fid/max(1,N), s_used/max(1,N)\n",
    "\n",
    "def train_seqT_minrot(n=6,depth=5,topology='star',pairs=None,epochs=20,train_size=60000,val_size=8000,batch=4096,lr=2e-3,p_continue=0.4,lam_use=0.02,h=2048,seed=0,device='cuda',num_workers=0,val_max_batches=20):\n",
    "    torch.manual_seed(seed); np.random.seed(seed)\n",
    "    torch.backends.cudnn.benchmark=True\n",
    "    if torch.cuda.is_available(): torch.set_float32_matmul_precision('high')\n",
    "    if pairs is None: pairs=make_pairs(n,topology)\n",
    "    P=len(pairs); STOP=P\n",
    "    train_ds=SeqTDataset(n,depth,pairs,train_size,p_continue=p_continue,seed=seed)\n",
    "    val_ds=SeqTDataset(n,depth,pairs,val_size,p_continue=p_continue,seed=seed+1)\n",
    "    in_dim=2*n*n\n",
    "    model=SeqTNet(in_dim,P,depth,h).to(device)\n",
    "    opt=torch.optim.AdamW(model.parameters(),lr=lr)\n",
    "    scaler=GradScaler(enabled=(device.startswith('cuda') and torch.cuda.is_available()))\n",
    "    train_loader=DataLoader(train_ds,batch_size=batch,shuffle=True,drop_last=True,num_workers=num_workers,persistent_workers=(num_workers>0),pin_memory=True)\n",
    "    val_loader=DataLoader(val_ds,batch_size=batch,shuffle=False,num_workers=num_workers,persistent_workers=False,pin_memory=True)\n",
    "    best_fid=-1.0; best_state=None\n",
    "    for ep in range(1,epochs+1):\n",
    "        model.train()\n",
    "        tot=0.0; cnt=0\n",
    "        for xb,y_pairs,y_angles in train_loader:\n",
    "            xb=xb.pin_memory().to(device, non_blocking=True).float()\n",
    "            y_pairs=y_pairs.pin_memory().to(device, non_blocking=True).long()\n",
    "            y_angles=y_angles.pin_memory().to(device, non_blocking=True).float()\n",
    "            with autocast(enabled=(device.startswith('cuda') and torch.cuda.is_available())):\n",
    "                L,A=model(xb)\n",
    "                depth_=L.size(1)\n",
    "                ce=0.0\n",
    "                for t in range(depth_):\n",
    "                    ce+=torch.nn.functional.cross_entropy(L[:,t,:],y_pairs[:,t])\n",
    "                idx=torch.clamp(y_pairs,0,STOP-1).unsqueeze(-1).unsqueeze(-1).expand(-1,-1,1,4)\n",
    "                sel=torch.gather(A,2,idx).squeeze(2)\n",
    "                mask=(y_pairs!=STOP).float().unsqueeze(-1)\n",
    "                mse=torch.nn.functional.mse_loss(sel,y_angles,reduction='none')\n",
    "                mse=(mse*mask).mean()\n",
    "                probs=torch.softmax(L,dim=-1)\n",
    "                use_pen=(1.0-probs[:,:,STOP]).mean()\n",
    "                loss=ce/depth_+mse+lam_use*use_pen\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update(); opt.zero_grad(set_to_none=True)\n",
    "            tot+=loss.detach().float().item()*xb.size(0); cnt+=xb.size(0)\n",
    "        train_loss=tot/cnt\n",
    "        vloss,vfid,vused=eval_stats(model,val_loader,n,pairs,STOP,device,max_batches=val_max_batches)\n",
    "        if vfid>best_fid:\n",
    "            best_fid=vfid\n",
    "            best_state={k:v.detach().cpu() for k,v in model.state_dict().items()}\n",
    "        print(f\"epoch {ep:02d} | train_loss {train_loss:.5f} val_loss {vloss:.5f} | val_fid {vfid:.6f} | val_used {vused:.3f}\")\n",
    "    model.load_state_dict(best_state)\n",
    "    meta={'n':n,'pairs':pairs,'depth':depth,'STOP':STOP,'h':h}\n",
    "    torch.save({'state_dict':model.state_dict(),'meta':meta},'seqT_minrot.pt')\n",
    "    return model,meta\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_seqT(model,meta,U):\n",
    "    n=meta['n']; pairs=meta['pairs']; depth=meta['depth']; STOP=meta['STOP']\n",
    "    x=torch.from_numpy(vec_features(canon_phase(U))).unsqueeze(0).float()\n",
    "    L,A=model(x)\n",
    "    L=L[0]; A=A[0]\n",
    "    seq=[]; ths=[]; phs=[]\n",
    "    for t in range(depth):\n",
    "        k=int(torch.argmax(L[t]).item())\n",
    "        if k==STOP: break\n",
    "        y=A[t,k].numpy()\n",
    "        th,ph=unpack_angles(y)\n",
    "        seq.append({'pair':pairs[k],'theta':float(th),'phi':float(ph)})\n",
    "        ths.append(th); phs.append(ph)\n",
    "    Uhat=canon_phase(apply_seq(n,pairs,[s['pair'] for s in seq],np.array(ths) if ths else np.array([],dtype=float),np.array(phs) if phs else np.array([],dtype=float)))\n",
    "    return seq,Uhat\n",
    "\n",
    "def demo_seqT(n=6,depth=5,topology='star',pairs=None,seed=123):\n",
    "    if pairs is None: pairs=make_pairs(n,topology)\n",
    "    rng=np.random.default_rng(seed)\n",
    "    L=4\n",
    "    for t in range(depth-1):\n",
    "        if rng.random()<0.2: L+=1\n",
    "        else: break\n",
    "    ks=[int(rng.integers(0,len(pairs))) for _ in range(L)]\n",
    "    ths=[rng.uniform(0,math.pi/2) for _ in range(L)]\n",
    "    phs=[rng.uniform(0,2*math.pi) for _ in range(L)]\n",
    "    U=canon_phase(apply_seq(n,pairs,[pairs[k] for k in ks],np.array(ths),np.array(phs)))\n",
    "    print(U)\n",
    "    ckpt=torch.load('seqT_minrot.pt',map_load_map='cpu') if False else torch.load('seqT_minrot.pt',map_location='cpu')\n",
    "    in_dim=2*n*n; P=len(pairs)\n",
    "    model=SeqTNet(in_dim,P,depth,ckpt['meta'].get('h',512))\n",
    "    model.load_state_dict(ckpt['state_dict']); model.eval()\n",
    "    seq_pred,U_pred=predict_seqT(model,{'n':n,'pairs':pairs,'depth':depth,'STOP':P},U)\n",
    "    print(U_pred)\n",
    "    fid=fidelity(U,U_pred)\n",
    "    return {'true_len':L,'true_seq':[{'pair':pairs[ks[i]],'theta':float(ths[i]),'phi':float(phs[i])} for i in range(L)],'pred_seq':seq_pred,'fid':fid,'U_true':U,'U_pred':U_pred}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba2cfecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iamga\\AppData\\Local\\Temp\\ipykernel_73436\\288137446.py:198: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler=GradScaler(enabled=(device.startswith('cuda') and torch.cuda.is_available()))\n",
      "C:\\Users\\iamga\\AppData\\Local\\Temp\\ipykernel_73436\\288137446.py:209: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(device.startswith('cuda') and torch.cuda.is_available())):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01 | train_loss 0.70021 val_loss 0.42421 | val_fid 0.832150 | val_used 0.335\n",
      "epoch 02 | train_loss 0.35375 val_loss 0.31391 | val_fid 0.908742 | val_used 0.327\n",
      "epoch 03 | train_loss 0.27987 val_loss 0.27102 | val_fid 0.925020 | val_used 0.323\n",
      "epoch 04 | train_loss 0.24041 val_loss 0.24134 | val_fid 0.930382 | val_used 0.335\n",
      "epoch 05 | train_loss 0.22443 val_loss 0.22520 | val_fid 0.939794 | val_used 0.330\n",
      "epoch 06 | train_loss 0.20476 val_loss 0.21419 | val_fid 0.939902 | val_used 0.329\n",
      "epoch 07 | train_loss 0.19318 val_loss 0.20350 | val_fid 0.943523 | val_used 0.332\n",
      "epoch 08 | train_loss 0.18220 val_loss 0.20225 | val_fid 0.944295 | val_used 0.333\n",
      "epoch 09 | train_loss 0.17848 val_loss 0.19692 | val_fid 0.944152 | val_used 0.328\n",
      "epoch 10 | train_loss 0.16945 val_loss 0.19038 | val_fid 0.945609 | val_used 0.332\n",
      "epoch 11 | train_loss 0.16543 val_loss 0.18780 | val_fid 0.946885 | val_used 0.331\n",
      "epoch 12 | train_loss 0.16074 val_loss 0.18228 | val_fid 0.947518 | val_used 0.332\n",
      "epoch 13 | train_loss 0.15549 val_loss 0.18430 | val_fid 0.949743 | val_used 0.329\n",
      "epoch 14 | train_loss 0.15005 val_loss 0.18448 | val_fid 0.951405 | val_used 0.335\n",
      "epoch 15 | train_loss 0.14906 val_loss 0.18030 | val_fid 0.950633 | val_used 0.331\n",
      "epoch 16 | train_loss 0.14272 val_loss 0.17566 | val_fid 0.952083 | val_used 0.332\n",
      "epoch 17 | train_loss 0.14249 val_loss 0.18368 | val_fid 0.948930 | val_used 0.333\n",
      "epoch 18 | train_loss 0.13885 val_loss 0.17979 | val_fid 0.950054 | val_used 0.333\n",
      "epoch 19 | train_loss 0.13526 val_loss 0.17641 | val_fid 0.953797 | val_used 0.329\n",
      "epoch 20 | train_loss 0.13116 val_loss 0.18371 | val_fid 0.951864 | val_used 0.330\n"
     ]
    }
   ],
   "source": [
    "n=6\n",
    "model, meta = train_seqT_minrot(n=n, depth=5, topology='star', epochs=20, train_size=300000, val_size=8000, batch=4096, lr=6e-3, p_continue=0.4, lam_use=0.005, h=2048, seed=0, device='cuda', num_workers=0, val_max_batches=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d32e90df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.3145-0.1473j  0.7214-0.1587j  0.    +0.j      0.5244+0.2426j\n",
      "   0.    +0.j      0.    +0.j    ]\n",
      " [ 0.0305+0.9044j  0.2703+0.3287j  0.    +0.j      0.    +0.j\n",
      "   0.    +0.j      0.    +0.j    ]\n",
      " [ 0.    +0.j      0.    +0.j      1.    -0.j      0.    +0.j\n",
      "   0.    +0.j      0.    +0.j    ]\n",
      " [-0.062 +0.2379j -0.2035-0.4817j  0.    +0.j     -0.2021+0.7908j\n",
      "   0.    +0.j      0.    +0.j    ]\n",
      " [ 0.    +0.j      0.    +0.j      0.    +0.j      0.    +0.j\n",
      "   1.    -0.j      0.    +0.j    ]\n",
      " [ 0.    +0.j      0.    +0.j      0.    +0.j      0.    +0.j\n",
      "   0.    +0.j      1.    -0.j    ]]\n",
      "[[ 0.0748-0.0053j  0.2097+0.3682j  0.    +0.j     -0.1928+0.2275j\n",
      "  -0.0959+0.8466j  0.    +0.j    ]\n",
      " [-0.4252+0.8882j  0.1743-0.j      0.    +0.j      0.    +0.j\n",
      "   0.    +0.j      0.    +0.j    ]\n",
      " [ 0.    +0.j      0.    +0.j      1.    -0.j      0.    +0.j\n",
      "   0.    +0.j      0.    +0.j    ]\n",
      " [ 0.0151+0.0179j -0.0541+0.1208j  0.    +0.j      0.9521+0.0679j\n",
      "  -0.2311+0.1321j  0.    +0.j    ]\n",
      " [ 0.0285+0.1529j -0.71  +0.5182j  0.    +0.j      0.    +0.j\n",
      "   0.4508-0.j      0.    +0.j    ]\n",
      " [ 0.    +0.j      0.    +0.j      0.    +0.j      0.    +0.j\n",
      "   0.    +0.j      1.    -0.j    ]]\n",
      "{\n",
      "  \"true_len\": 4,\n",
      "  \"true_seq\": [\n",
      "    {\n",
      "      \"pair\": [\n",
      "        0,\n",
      "        1\n",
      "      ],\n",
      "      \"theta\": 0.410732073284677,\n",
      "      \"phi\": 2.5906367578188676\n",
      "    },\n",
      "    {\n",
      "      \"pair\": [\n",
      "        0,\n",
      "        1\n",
      "      ],\n",
      "      \"theta\": 1.5224110777714333,\n",
      "      \"phi\": 4.763641170442362\n",
      "    },\n",
      "    {\n",
      "      \"pair\": [\n",
      "        0,\n",
      "        3\n",
      "      ],\n",
      "      \"theta\": 1.4826487876138985,\n",
      "      \"phi\": 3.453723021429006\n",
      "    },\n",
      "    {\n",
      "      \"pair\": [\n",
      "        0,\n",
      "        3\n",
      "      ],\n",
      "      \"theta\": 0.9841620329506636,\n",
      "      \"phi\": 4.717400511291818\n",
      "    }\n",
      "  ],\n",
      "  \"pred_seq\": [\n",
      "    {\n",
      "      \"pair\": [\n",
      "        0,\n",
      "        1\n",
      "      ],\n",
      "      \"theta\": 1.3956481218338013,\n",
      "      \"phi\": 4.265863418579102\n",
      "    },\n",
      "    {\n",
      "      \"pair\": [\n",
      "        0,\n",
      "        4\n",
      "      ],\n",
      "      \"theta\": 1.1031246185302734,\n",
      "      \"phi\": 4.896343231201172\n",
      "    },\n",
      "    {\n",
      "      \"pair\": [\n",
      "        0,\n",
      "        3\n",
      "      ],\n",
      "      \"theta\": 0.2053138017654419,\n",
      "      \"phi\": 3.848527193069458\n",
      "    },\n",
      "    {\n",
      "      \"pair\": [\n",
      "        0,\n",
      "        3\n",
      "      ],\n",
      "      \"theta\": 0.34592723846435547,\n",
      "      \"phi\": 5.938714504241943\n",
      "    },\n",
      "    {\n",
      "      \"pair\": [\n",
      "        0,\n",
      "        3\n",
      "      ],\n",
      "      \"theta\": 0.029071569442749023,\n",
      "      \"phi\": 0.2708068788051605\n",
      "    }\n",
      "  ],\n",
      "  \"fid\": 0.5216481879868247\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "res = demo_seqT(n=n, depth=5, topology='star', seed=3123)\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "print(json.dumps({k:res[k] for k in ['true_len','true_seq','pred_seq','fid']}, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "da43f45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.    -0.j -0.    +0.j -0.    +0.j -0.    +0.j  0.    +0.j  0.    +0.j]\n",
      " [ 0.    +0.j  0.5732+0.j -0.7392-0.j -0.3536+0.j  0.    +0.j  0.    +0.j]\n",
      " [ 0.    +0.j -0.7392+0.j -0.2803+0.j -0.6124+0.j  0.    +0.j  0.    +0.j]\n",
      " [ 0.    +0.j -0.3536-0.j -0.6124-0.j  0.7071+0.j  0.    +0.j  0.    +0.j]\n",
      " [ 0.    +0.j  0.    +0.j  0.    +0.j  0.    +0.j  1.    +0.j  0.    +0.j]\n",
      " [ 0.    +0.j  0.    +0.j  0.    +0.j  0.    +0.j  0.    +0.j  1.    +0.j]]\n",
      "[[ 0.5+0.j  0.5+0.j  0.5+0.j  0.5+0.j]\n",
      " [ 0.5+0.j -0.5+0.j  0.5+0.j -0.5+0.j]\n",
      " [ 0.5+0.j  0.5+0.j -0.5+0.j -0.5+0.j]\n",
      " [ 0.5+0.j -0.5+0.j -0.5+0.j  0.5-0.j]]\n",
      "[[ 0.5+0.j   0.5+0.j   0.5+0.j   0.5+0.j   0. +0.j   0. +0.j ]\n",
      " [ 0.5+0.j   0. +0.5j -0.5+0.j  -0. -0.5j  0. +0.j   0. +0.j ]\n",
      " [ 0.5+0.j  -0.5+0.j   0.5-0.j  -0.5+0.j   0. +0.j   0. +0.j ]\n",
      " [ 0.5+0.j  -0. -0.5j -0.5+0.j   0. +0.5j  0. +0.j   0. +0.j ]\n",
      " [ 0. +0.j   0. +0.j   0. +0.j   0. +0.j   1. +0.j   0. +0.j ]\n",
      " [ 0. +0.j   0. +0.j   0. +0.j   0. +0.j   0. +0.j   1. +0.j ]]\n",
      "5 [(0, 1), (0, 2), (0, 3), (0, 4), (0, 5)]\n",
      "[[ 0.1487+0.0108j -0.0642+0.8116j  0.    +0.j     -0.4919+0.2702j\n",
      "   0.    +0.j      0.    +0.j    ]\n",
      " [ 0.0062+0.9836j  0.1802+0.j      0.    +0.j      0.    +0.j\n",
      "   0.    +0.j      0.    +0.j    ]\n",
      " [ 0.    +0.j      0.    +0.j      1.    +0.j      0.    +0.j\n",
      "   0.    +0.j      0.    +0.j    ]\n",
      " [ 0.0886+0.0487j -0.2688+0.4822j  0.    +0.j      0.8255-0.0601j\n",
      "   0.    +0.j      0.    +0.j    ]\n",
      " [ 0.    +0.j      0.    +0.j      0.    +0.j      0.    +0.j\n",
      "   1.    +0.j      0.    +0.j    ]\n",
      " [ 0.    +0.j      0.    +0.j      0.    +0.j      0.    +0.j\n",
      "   0.    +0.j      1.    +0.j    ]]\n",
      "{\n",
      "  \"true_seq\": [\n",
      "    {\n",
      "      \"pair\": [\n",
      "        0,\n",
      "        1\n",
      "      ],\n",
      "      \"theta\": 1.5707963267948966,\n",
      "      \"phi\": 5.1\n",
      "    },\n",
      "    {\n",
      "      \"pair\": [\n",
      "        0,\n",
      "        2\n",
      "      ],\n",
      "      \"theta\": 1.0471975511965976,\n",
      "      \"phi\": 5.1\n",
      "    },\n",
      "    {\n",
      "      \"pair\": [\n",
      "        0,\n",
      "        3\n",
      "      ],\n",
      "      \"theta\": 0.7853981633974483,\n",
      "      \"phi\": 5.1\n",
      "    },\n",
      "    {\n",
      "      \"pair\": [\n",
      "        0,\n",
      "        2\n",
      "      ],\n",
      "      \"theta\": 1.0471975511965976,\n",
      "      \"phi\": 5.1\n",
      "    },\n",
      "    {\n",
      "      \"pair\": [\n",
      "        0,\n",
      "        1\n",
      "      ],\n",
      "      \"theta\": 1.5707963267948966,\n",
      "      \"phi\": 5.1\n",
      "    }\n",
      "  ],\n",
      "  \"pred_seq\": [\n",
      "    {\n",
      "      \"pair\": [\n",
      "        0,\n",
      "        1\n",
      "      ],\n",
      "      \"theta\": 1.3896211385726929,\n",
      "      \"phi\": 4.718667030334473\n",
      "    },\n",
      "    {\n",
      "      \"pair\": [\n",
      "        0,\n",
      "        3\n",
      "      ],\n",
      "      \"theta\": 0.00701749324798584,\n",
      "      \"phi\": 5.861433982849121\n",
      "    },\n",
      "    {\n",
      "      \"pair\": [\n",
      "        0,\n",
      "        3\n",
      "      ],\n",
      "      \"theta\": 0.21389877796173096,\n",
      "      \"phi\": 6.27241849899292\n",
      "    },\n",
      "    {\n",
      "      \"pair\": [\n",
      "        0,\n",
      "        3\n",
      "      ],\n",
      "      \"theta\": 0.2658787965774536,\n",
      "      \"phi\": 5.5676069259643555\n",
      "    },\n",
      "    {\n",
      "      \"pair\": [\n",
      "        0,\n",
      "        3\n",
      "      ],\n",
      "      \"theta\": 0.15125560760498047,\n",
      "      \"phi\": 5.500954627990723\n",
      "    }\n",
      "  ],\n",
      "  \"fid\": 0.3526188331376032\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "ckpt = torch.load('seqT_minrot.pt', map_location='cpu')\n",
    "pairs = ckpt['meta']['pairs']; n = ckpt['meta']['n']; depth = ckpt['meta']['depth']; h = ckpt['meta'].get('h',512)\n",
    "\n",
    "model = SeqTNet(2*n*n, len(pairs), depth, h)\n",
    "model.load_state_dict(ckpt['state_dict']); model.eval()\n",
    "\n",
    "seq_true = [\n",
    "    {'pair': (0,1), 'theta': math.pi/2, 'phi': 5.1},\n",
    "    {'pair': (0,2), 'theta': math.pi/3, 'phi': 5.1},\n",
    "    {'pair': (0,3), 'theta': math.pi/4, 'phi': 5.1},\n",
    "    {'pair': (0,2), 'theta': math.pi/3, 'phi': 5.1},\n",
    "    {'pair': (0,1), 'theta': math.pi/2, 'phi': 5.1},\n",
    "]\n",
    "\n",
    "seq_pairs = [s['pair'] for s in seq_true]\n",
    "ths = np.array([s['theta'] for s in seq_true])\n",
    "phs = np.array([s['phi'] for s in seq_true])\n",
    "\n",
    "U = canon_phase(apply_seq(n, pairs, seq_pairs, ths, phs))\n",
    "print(U)\n",
    "H2 = (1/np.sqrt(2))*np.array([[1,1],[1,-1]], dtype=complex)\n",
    "U = np.kron(H2, H2)\n",
    "print(U)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def H4_dft():\n",
    "    j,k=np.ogrid[:4,:4]\n",
    "    omega=np.exp(2j*np.pi/4)\n",
    "    return (omega**(j*k))/2\n",
    "\n",
    "def H4_real():\n",
    "    return 0.5*np.array([[1,1,1,1],\n",
    "                         [1,-1,1,-1],\n",
    "                         [1,1,-1,-1],\n",
    "                         [1,-1,-1,1]],dtype=float)\n",
    "\n",
    "def embed_4_in_6(H4, idx=(0,1,2,3)):\n",
    "    U=np.eye(6, dtype=complex)\n",
    "    idx=list(idx)\n",
    "    comp=[x for x in range(6) if x not in idx]\n",
    "    P=np.eye(6)[:, idx+comp]\n",
    "    B=np.block([[H4, np.zeros((4,2))],\n",
    "                [np.zeros((2,4)), np.eye(2)]])\n",
    "    return P @ B @ P.T\n",
    "\n",
    "U = embed_4_in_6(H4_dft(), idx=(0,1,2,3))\n",
    "# or: H6 = embed_4_in_6(H4_real(), idx=(0,1,2,3))\n",
    "print(U)\n",
    "\n",
    "print(len(pairs), pairs)\n",
    "seq_pred, U_pred = predict_seqT(model, {'n':n,'pairs':pairs,'depth':depth,'STOP':len(pairs)}, U)\n",
    "fid = fidelity(U, U_pred)\n",
    "print(U_pred)\n",
    "print(json.dumps({'true_seq':seq_true,'pred_seq':seq_pred,'fid':fid}, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc116285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5+0.j  0.5+0.j  0.5+0.j  0.5+0.j]\n",
      " [ 0.5+0.j -0.5+0.j  0.5+0.j -0.5+0.j]\n",
      " [ 0.5+0.j  0.5+0.j -0.5+0.j -0.5+0.j]\n",
      " [ 0.5+0.j -0.5+0.j -0.5+0.j  0.5-0.j]]\n"
     ]
    }
   ],
   "source": [
    "H2 = (1/np.sqrt(2))*np.array([[1,1],[1,-1]], dtype=complex)\n",
    "U = np.kron(H2, H2)\n",
    "print(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f65bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100e5830",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
